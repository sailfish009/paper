1, TITLE: Lightweight Design of Rear Seats of A Passenger Car Based on Topology Optimization of Equivalent Static Load Method and Multi-objective Size Optimization
AUTHORS: Yaoqing Liao; Xuan Zhou; Hengliang Jiang
CATEGORY: Proceedings of the Institution of Mechanical Engineers, ... [PROCEEDINGS OF THE INSTITUTION OF MECHANICAL ENGINEERS, ...]
HIGHLIGHT: This study addresses the trade-off between crash safety and lightweight performance in rear automotive seat design by proposing a two-stage optimization framework that integrates equivalent static load-based topology optimization with multi-objective size optimization.

2, TITLE: Agentic Misalignment: How LLMs Could Be Insider Threats
AUTHORS: Aengus Lynch; Benjamin Wright; Caleb Larson; Stuart J. Ritchie; Soren Mindermann; Evan Hubinger; Ethan Perez; Kevin Troy
CATEGORY: arxiv-cs.CR [CR]
HIGHLIGHT: Abstract: We stress-tested 16 leading models from multiple developers in hypotheticalcorporate environments to identify potentially risky agentic behaviors beforethey cause real harm. In ...

3, TITLE: Internal States Before Wait Modulate Reasoning Patterns
AUTHORS: Dmitrii Troitskii; Koyena Pal; Chris Wendler; Callum Stuart McDougall; Neel Nanda
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: In this work, we address the question whether model'slatents preceding wait tokens contain relevant information for modulating thesubsequent reasoning process.

4, TITLE: ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation
AUTHORS: Jay Zhangjie Wu; Xuanchi Ren; Tianchang Shen; Tianshi Cao; Kai He; Yifan Lu; Ruiyuan Gao; Enze Xie; Shiyi Lan; Jose M. Alvarez; Jun Gao; Sanja Fidler; Zian Wang; Huan Ling
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: In thispaper, we present ChronoEdit, a framework that reframes image editing as avideo generation problem.

5, TITLE: COSMO-RL: Towards Trustworthy LMRMs Via Joint Safety and Stability
AUTHORS: Yizhuo Ding; Mingkang Chen; Qiuhua Liu; Fenghua Weng; Wanying Qu; Yue Yang; Yugang Jiang; Zuxuan Wu; Yanwei Fu; Wenqi Shao
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: We present COSMO-RL, a mixedreinforcement learning framework that trains reasoning oriented LMRMs undermultimodal, multitask, and multiobjective signals, and we release the resultingmodel, COSMO-R1.

6, TITLE: ID-Consistent, Precise Expression Generation with Blendshape-Guided Diffusion
AUTHORS: Foivos Paraperas Papantoniou; Stefanos Zafeiriou
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: In thiswork, we present a diffusion-based framework that faithfully reimagines anysubject under any particular facial expression.

7, TITLE: Factuality Matters: When Image Generation and Editing Meet Structured Visuals
AUTHORS: Le Zhuo; Songhao Han; Yuandong Pu; Boxiang Qiu; Sayak Paul; Yue Liao; Yihao Liu; Jie Shao; Xi Chen; Si Liu; Hongsheng Li
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: By releasing the dataset,model, and benchmark, we aim to advance unified multimodal foundations forstructured visuals.

8, TITLE: GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks
AUTHORS: Tejal Patwardhan; Rachel Dias; Elizabeth Proehl; Grace Kim; Michele Wang; Olivia Watkins; Simón Posada Fishman; Marwan Aljubeh; Phoebe Thacker; Laurance Fauconnet; Natalie S. Kim; Patrick Chao; Samuel Miserendino; Gildas Chabot; David Li; Michael Sharman; Alexandra Barr; Amelia Glaese; Jerry Tworek
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: We introduce GDPval, a benchmark evaluating AI model capabilities onreal-world economically valuable tasks.

9, TITLE: Multi-Agent Tool-Integrated Policy Optimization
AUTHORS: Zhanfeng Mo; Xingxuan Li; Yuntao Chen; Lidong Bing
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: A natural solution is to adopt amulti-agent framework with planner- and worker-agents to manage context.However, no existing methods support effective reinforcement learningpost-training of tool-integrated multi-agent frameworks. To address this gap,we propose Multi-Agent Tool-Integrated Policy Optimization (MATPO), whichenables distinct roles (planner and worker) to be trained within a single LLMinstance using role-specific prompts via reinforcement learning.

10, TITLE: Offline Reinforcement Learning in Large State Spaces: Algorithms and Guarantees
AUTHORS: Nan Jiang; Tengyang Xie
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: This article introduces the theory of offline reinforcement learning in largestate spaces, where good policies are learned from historical data withoutonline interactions with the environment.

11, TITLE: LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization
AUTHORS: Xueyang Zhou; Yangming Xu; Guiyao Tie; Yongchao Chen; Guowen Zhang; Duanfeng Chu; Pan Zhou; Lichao Sun
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: Abstract: LIBERO has emerged as a widely adopted benchmark for evaluatingVision-Language-Action (VLA) models; however, its current training andevaluation settings are problematic, often ...

12, TITLE: From Noisy Traces to Stable Gradients: Bias-Variance Optimized Preference Optimization for Aligning Large Reasoning Models
AUTHORS: Mingkang Zhu; Xi Chen; Bei Yu; Hengshuang Zhao; Jiaya Jia
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: A common workaround optimizes a singlesampled trajectory, which introduces substantial gradient variance fromstochastic trace sampling. To address this challenge, we frame preferenceoptimization for LRMs through the lens of the bias--variance trade-off andpropose Bias--Variance Optimized Preference Optimization (BVPO), a simple,drop-in method that mixes two gradient estimators: a high-variance trace-basedestimator and a low-variance empty-trace estimator obtained by disablingreasoning trace generation.

13, TITLE: Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off The Rails
AUTHORS: Siwei Han; Jiaqi Liu; Yaofeng Su; Wenbo Duan; Xinyuan Liu; Cihang Xie; Mohit Bansal; Mingyu Ding; Linjun Zhang; Huaxiu Yao
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: Abstract: As Large Language Model (LLM) agents increasingly gain self-evolutionarycapabilities to adapt and refine their strategies through real-worldinteraction, their long-term ...

14, TITLE: LLM, Reporting In! Medical Information Extraction Across Prompting, Fine-tuning and Post-correction
AUTHORS: Ikram Belmadani; Parisa Nazari Hashemi; Thomas Sebbag; Benoit Favre; Guillaume Fortier; Solen Quiniou; Emmanuel Morin; Richard Dufour
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: For NER, we propose three approaches combining largelanguage models (LLMs), annotation guidelines, synthetic data, andpost-processing: (1) in-context learning (ICL) with GPT-4.1, incorporatingautomatic selection of 10 examples and a summary of the annotation guidelinesinto the prompt, (2) the universal NER system GLiNER, fine-tuned on a syntheticcorpus and then verified by an LLM in post-processing, and (3) the open LLMLLaMA-3.1-8B-Instruct, fine-tuned on the same synthetic corpus.

15, TITLE: Hybrid Architectures for Language Models: Systematic Analysis and Design Insights
AUTHORS: Sangmin Bae; Bilge Acun; Haroun Habeeb; Seungyeon Kim; Chien-Yu Lin; Liang Luo; Junjie Wang; Carole-Jean Wu
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: In this work, wepresent a holistic evaluation of hybrid architectures based on inter-layer(sequential) or intra-layer (parallel) fusion.

16, TITLE: AgentRL: Scaling Agentic Reinforcement Learning with A Multi-Turn, Multi-Task Framework
AUTHORS: Hanchen Zhang; Xiao Liu; Bowen Lv; Xueqiao Sun; Bohao Jing; Iat Long Iong; Zhenyu Hou; Zehan Qi; Hanyu Lai; Yifan Xu; Rui Lu; Hongning Wang; Jie Tang; Yuxiao Dong
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: In this work, we present theAgentRL framework for scalable multi-turn, multi-task agentic RL training.

17, TITLE: Just-in-time Episodic Feedback Hinter: Leveraging Offline Knowledge to Improve LLM Agents Adaptation
AUTHORS: Hadi Nekoei; Aman Jaiswal; Patrice Bechard; Oleh Shliazhko; Orlando Marquez Ayala; Mathieu Reymond; Massimo Caccia; Alexandre Drouin; Sarath Chandar; Alexandre Lacoste
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: We present Just-in-time EpisodicFeedback Hinter (JEF Hinter), an agentic system that distills offline tracesinto compact, context-aware hints.

18, TITLE: Watch and Learn: Learning to Use Computers from Online Videos
AUTHORS: Chan Hee Song; Yiwen Song; Palash Goyal; Yu Su; Oriana Riva; Hamid Palangi; Tomas Pfister
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: Computer use agents (CUAs) need to plan task workflows grounded in diverse,ever-changing applications and environments, but learning is hindered by thescarcity of large-scale, high-quality training data in the target application.Existing datasets are domain-specific, static, and costly to annotate, whilecurrent synthetic data generation methods often yield simplistic or misalignedtask demonstrations. To address these limitations, we introduce Watch & Learn(W&L), a framework that converts human demonstration videos readily availableon the Internet into executable UI trajectories at scale.

19, TITLE: CALM Before The STORM: Unlocking Native Reasoning for Optimization Modeling
AUTHORS: Zhengyang Tang; Zihan Ye; Chenyu Huang; Xuhan Huang; Chengpeng Li; Sihang Li; Guanhua Chen; Ming Yan; Zizhuo Wang; Hongyuan Zha; Dayiheng Liu; Benyou Wang
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: To fully leverage LRMs' inherent reasoning abilities, we propose\textbf{CALM} (\textit{Corrective Adaptation with Lightweight Modification}), aframework that progressively refines LRMs within their native reasoning modesfor optimization modeling tasks.

20, TITLE: Visual Representations Inside The Language Model
AUTHORS: Benlin Liu; Amita Kamath; Madeleine Grunde-McLaughlin; Winson Han; Ranjay Krishna
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: We offer an under-studied perspective byexamining how popular MLMs (LLaVA-OneVision, Qwen2.5-VL, andLlama-3-LLaVA-NeXT) process their visual key-value tokens.

21, TITLE: SAEdit: Token-level Control for Continuous Image Editing Via Sparse AutoEncoder
AUTHORS: Ronen Kamenetsky; Sara Dorfman; Daniel Garibi; Roni Paiss; Or Patashnik; Daniel Cohen-Or
CATEGORY: arxiv-cs.GR [GR]
HIGHLIGHT: The edits are applied by manipulating theembeddings along carefully chosen directions, which control the strength of thetarget attribute. To identify such directions, we employ a Sparse Autoencoder(SAE), whose sparse latent space exposes semantically isolated dimensions.

22, TITLE: Paper2Video: Automatic Video Generation from Scientific Papers
AUTHORS: Zeyu Zhu; Kevin Qinghong Lin; Mike Zheng Shou
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: Unlike natural video, presentation video generation involvesdistinctive challenges: inputs from research papers, dense multi-modalinformation (text, figures, tables), and the need to coordinate multiplealigned channels such as slides, subtitles, speech, and human talker. Toaddress these challenges, we introduce Paper2Video, the first benchmark of 101research papers paired with author-created presentation videos, slides, andspeaker metadata.

23, TITLE: Staircase Streaming for Low-Latency Multi-Agent Inference
AUTHORS: Junlin Wang; Jue Wang; Ben Athiwaratkun; Bhuwan Dhingra; Ce Zhang; James Zou
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: Whilemulti-agent inference can enhance response quality, it can significantlyincrease the time to first token (TTFT), posing a challenge forlatency-sensitive applications and hurting user experience. To address thisissue, we propose staircase streaming for low-latency multi-agent inference.Instead of waiting for the complete intermediate outputs from previous steps,we begin generating the final response as soon as we receive partial outputsfrom these steps.

24, TITLE: On Structured State-Space Duality
AUTHORS: Jerry Yao-Chieh Hu; Xiwen Zhang; Weimin Wu; Han Liu
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: Consequently, the same sequence transformation (model) has twoalgorithmic realizations: as a linear-time $O(T)$ recurrence or as aquadratic-time $O(T^2)$ attention. In this note, we formalize and generalizethis duality: (i) we extend SSD from the scalar-identity case to generaldiagonal SSMs (diagonal state matrices); (ii) we show that these diagonal SSMsmatch the scalar case's training complexity lower bounds while supportingricher dynamics; (iii) we establish a necessary and sufficient condition underwhich an SSM is equivalent to $1$-semiseparable masked attention; and (iv) weshow that such duality fails to extend to standard softmax attention due torank explosion.

25, TITLE: Imperceptible Jailbreaking Against Large Language Models
AUTHORS: Kuofeng Gao; Yiming Li; Chao Du; Xin Wang; Xingjun Ma; Shu-Tao Xia; Tianyu Pang
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: In this paper, we introduce imperceptible jailbreaks that exploit aclass of Unicode characters called variation selectors.

26, TITLE: Finish First, Perfect Later: Test-Time Token-Level Cross-Validation for Diffusion Large Language Models
AUTHORS: Runchu Tian; Junxia Cui; Xueqiang Xu; Feng Yao; Jingbo Shang
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: We evaluate Tolerator on five standardbenchmarks covering language understanding, code generation, and mathematics.Experiments show that our method achieves consistent improvements over thebaselines under the same computational budget.

27, TITLE: Compressed Convolutional Attention: Efficient Attention in A Compressed Latent Space
AUTHORS: Tomas Figliolia; Nicholas Alonso; Rishi Iyer; Quentin Anthony; Beren Millidge
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: We introduce Compressed ConvolutionalAttention (CCA), a novel attention method which down-projects queries, keys,and values and performs the entire attention operation inside the shared latentspace.

28, TITLE: MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator
AUTHORS: Xuehai He; Shijie Zhou; Thivyanth Venkateswaran; Kaizhi Zheng; Ziyu Wan; Achuta Kadambi; Xin Eric Wang
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: We introduce MorphoSim, a language guided framework thatgenerates 4D scenes with multi-view consistency and object-level controls.

29, TITLE: Multi-Agent Collaborative Intelligence: Dual-Dial Control for Reliable LLM Reasoning
AUTHORS: Edward Y. Chang; Ethan Y. Chang
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: We introduce MACI,an active controller with two independent dials that decouple information frombehavior: an information dial that gates evidence by quality, and a behaviordial that schedules contentiousness from exploration to consolidation.

30, TITLE: OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows
AUTHORS: John Nguyen; Marton Havasi; Tariq Berrada; Luke Zettlemoyer; Ricky T. Q. Chen
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: We present OneFlow, the first non-autoregressive multimodal model thatenables variable-length and concurrent mixed-modal generation.

31, TITLE: Equipping Retrieval-Augmented Large Language Models with Document Structure Awareness
AUTHORS: Lingnan Xu; Chong Feng; Kaiyuan Zhang; Liu Zhengyong; Wenqiang Xu; Fanqing Meng
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: Abstract: While large language models (LLMs) demonstrate impressive capabilities, theirreliance on parametric knowledge often leads to factual inaccuracies.Retrieval-Augmented Generation ...

32, TITLE: LongTail-Swap: Benchmarking Language Models' Abilities on Rare Words
AUTHORS: Robin Algayres; Charles-Éric Saint-James; Mahi Luthra; Jiayi Shen; Dongyan Lin; Youssef Benchekroun; Rashel Moritz; Juan Pino; Emmanuel Dupoux
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: TheBabyLM challenge aims at exploring language model (LM) training in the low-dataregime but uses metrics that concentrate on the head of the word distribution.Here, we introduce LongTail-Swap (LT-Swap), a benchmark that focuses on thetail of the distribution, i.e., measures the ability of LMs to learn new wordswith very little exposure, like infants do.

33, TITLE: Efficient Prediction of Pass@k Scaling in Large Language Models
AUTHORS: Joshua Kazdan; Rylan Schaeffer; Youssef Allouah; Colin Sullivan; Kyssen Yu; Noam Levi; Sanmi Koyejo
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: Such results raise a crucial question for both capabilityand safety forecasting: how can one accurately predict a model's behavior whenscaled to a massive number of attempts, given a vastly smaller sampling budget?This question is directly relevant to model providers, who serve hundreds ofmillions of users daily, and to governmental regulators, who seek to preventharms. To answer this questions, we make three contributions. First, we findthat standard methods for fitting these laws suffer from statisticalshortcomings that hinder predictive accuracy, especially in data-limitedscenarios.

34, TITLE: VChain: Chain-of-Visual-Thought for Reasoning in Video Generation
AUTHORS: Ziqi Huang; Ning Yu; Gordon Chen; Haonan Qiu; Paul Debevec; Ziwei Liu
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: In contrast, large language andmultimodal models (e.g., GPT-4o) exhibit strong visual state reasoning andfuture prediction capabilities. To bridge these strengths, we introduce VChain,a novel inference-time chain-of-visual-thought framework that injects visualreasoning signals from multimodal models into video generation.

35, TITLE: Large Language Models Achieve Gold Medal Performance at International Astronomy & Astrophysics Olympiad
AUTHORS: Lucas Carrit Delgado Pinheiro; Ziru Chen; Bruno Caixeta Piazza; Ness Shroff; Yingbin Liang; Yuan-Sen Ting; Huan Sun
CATEGORY: arxiv-astro-ph.IM [IM]
HIGHLIGHT: So far, existing benchmarks and evaluations focus on simplequestion-answering that primarily tests astronomical knowledge and fails toevaluate the complex reasoning required for real-world research in thediscipline. Here, we address this gap by systematically benchmarking fivestate-of-the-art LLMs on the International Olympiad on Astronomy andAstrophysics (IOAA) exams, which are designed to examine deep conceptualunderstanding, multi-step derivations, and multimodal analysis.

36, TITLE: Learning from All: Concept Alignment for Autonomous Distillation from Multiple Drifting MLLMs
AUTHORS: Xiaoyu Yang; Jie Lu; En Yu
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: This paper identifies a critical yet underexplored challenge in distillingfrom multimodal large language models (MLLMs): the reasoning trajectoriesgenerated by multiple drifting teachers exhibit concept drift, whereby theirreasoning distributions evolve unpredictably and transmit biases to the studentmodel, ultimately compromising its performance. To tackle this issue, wepioneer a theoretical connection between concept drift and knowledgedistillation, casting the non-stationary reasoning dynamics from multiple MLLMteachers as next-token prediction of multi-stream reasoning trajectories.Guidedby concept drift, we introduce the "learn, compare, critique" paradigm,culminating in autonomous preference optimization (APO).

37, TITLE: SATER: A Self-Aware and Token-Efficient Approach to Routing and Cascading
AUTHORS: Yuanzhe Shen; Yide Liu; Zisu Huang; Ruicheng Yin; Xiaoqing Zheng; Xuanjing Huang
CATEGORY: arxiv-cs.DC [DC]
HIGHLIGHT: To further addressthe limitations of both approaches, we introduce SATER, a dual-mode compatibleapproach that fine-tunes models through shortest-response preferenceoptimization and a confidence-aware rejection mechanism.

38, TITLE: Automating Construction Safety Inspections Using A Multi-modal Vision-language RAG Framework
AUTHORS: Chenxin Wang; Elyas Asadi Shamsabadi; Zhaohui Chen; Luming Shen; Alireza Ahmadian Fard Fini; Daniel Dias-da-Costa
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: The findings indicate that SiteShieldoffers a novel pathway to enhance information retrieval and efficiency ingenerating safety reports.

39, TITLE: Streaming Drag-Oriented Interactive Video Manipulation: Drag Anything, Anytime!
AUTHORS: Junbao Zhou; Yuan Zhou; Kesen Zhao; Qingshan Xu; Beier Zhu; Richang Hong; Hanwang Zhang
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: In resolving REVEL, weobserve: \emph{i}) drag-induced perturbations accumulate in latent space,causing severe latent distribution drift that halts the drag process;\emph{ii}) streaming drag is easily disturbed by context frames, therebyyielding visually unnatural outcomes. We thus propose a training-free approach,\textbf{DragStream}, comprising: \emph{i}) an adaptive distributionself-rectification strategy that leverages neighboring frames' statistics toeffectively constrain the drift of latent embeddings; \emph{ii}) aspatial-frequency selective optimization mechanism, allowing the model to fullyexploit contextual information while mitigating its interference viaselectively propagating visual cues along generation.

40, TITLE: Learning on The Job: Test-Time Curricula for Targeted Reinforcement Learning
AUTHORS: Jonas Hübotter; Leander Diaz-Bone; Ido Hakimi; Andreas Krause; Moritz Hardt
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: Can a model do the same? We propose an agent thatassembles a task-specific curriculum, called test-time curriculum (TTC-RL), andapplies reinforcement learning to continue training the model for its targettask.

41, TITLE: Neon: Negative Extrapolation From Self-Training Improves Image Generation
AUTHORS: Sina Alemohammad; Zhangyang Wang; Richard G. Baraniuk
CATEGORY: arxiv-cs.GR [GR]
HIGHLIGHT: In this paper, we introduce Neon (for Negative ExtrapolationfrOm self-traiNing), a new learning method that turns the degradation fromself-training into a powerful signal for self-improvement.

42, TITLE: Cost Efficient Fairness Audit Under Partial Feedback
AUTHORS: Nirjhar Das; Mohit Sharma; Praharsh Nanavati; Kirankumar Shiragur; Amit Deshpande
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: We study the problem of auditing the fairness of a given classifier underpartial feedback, where true labels are available only for positivelyclassified individuals, (e.g., loan repayment outcomes are observed only forapproved applicants). We introduce a novel cost model for acquiring additionallabeled data, designed to more accurately reflect real-world costs such ascredit assessment, loan processing, and potential defaults.

43, TITLE: Generating Human Motion Videos Using A Cascaded Text-to-Video Framework
AUTHORS: Hyelin Nam; Hyojun Go; Byeongjun Park; Byung-Hoon Kim; Hyungjin Chung
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: Despite the rapidprogress of video diffusion models (VDMs), their use for general-purpose humanvideo generation remains underexplored, with most works constrained toimage-to-video setups or narrow domains like dance videos. In this work, wepropose CAMEO, a cascaded framework for general human motion video generation.It seamlessly bridges Text-to-Motion (T2M) models and conditional VDMs,mitigating suboptimal factors that may arise in this process across bothtraining and inference through carefully designed components.

44, TITLE: MITS: Enhanced Tree Search Reasoning for LLMs Via Pointwise Mutual Information
AUTHORS: Jiaxi Li; Yucheng Shi; Jin Lu; Ninghao Liu
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: However, it remains difficult to provide instant and reliablequantitative assessments of intermediate reasoning step quality, and extensivepath exploration is computationally costly. To address this, we propose MutualInformation Tree Search (MITS), a novel framework that guides reasoning withinformation-theoretic principles.

45, TITLE: A Novel Cloud-Based Diffusion-Guided Hybrid Model for High-Accuracy Accident Detection in Intelligent Transportation Systems
AUTHORS: Siva Sai; Saksham Gupta; Vinay Chamola; Rajkumar Buyya
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: We present anovel hybrid model integrating guidance classification with diffusiontechniques.

46, TITLE: Beyond Outcome Reward: Decoupling Search and Answering Improves LLM Agents
AUTHORS: Yiding Wang; Zhepei Wei; Xinyu Zhu; Yu Meng
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: Our analysis challenges thisassumption: we uncover multiple systematic deficiencies in search that ariseunder outcome-only training and ultimately degrade final answer quality,including failure to invoke tools, invalid queries, and redundant searches. Toaddress these shortcomings, we introduce DeSA (DecouplingSearch-and-Answering), a simple two-stage training framework that explicitlyseparates search optimization from answer generation.

47, TITLE: GRACE: Generative Representation Learning Via Contrastive Policy Optimization
AUTHORS: Jiashuo Sun; Shixuan Liu; Zhaochen Su; Xianrui Zhong; Pengcheng Jiang; Bowen Jin; Peiran Li; Weijia Shi; Jiawei Han
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: We introduce GRACE (Generative Representation Learning viaContrastive Policy Optimization), a novel framework that reimagines contrastivesignals not as losses to be minimized, but as rewards that guide a generativepolicy.

48, TITLE: Harnessing Synthetic Preference Data for Enhancing Temporal Understanding of Video-LLMs
AUTHORS: Sameep Vani; Shreyas Jena; Maitreya Patel; Chitta Baral; Somak Aditya; Yezhou Yang
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: In this work, we propose TimeWarp, asystematic method to create a targeted synthetic temporal dataset to fine-tunethe model's responses to encourage it to focus on the given input video.

49, TITLE: Speculative Actions: A Lossless Framework for Faster Agentic Systems
AUTHORS: Naimeng Ye; Arnav Ahuja; Georgios Liargkovas; Yunan Lu; Kostis Kaffes; Tianyi Peng
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: Inspired by speculative execution in microprocessors andspeculative decoding in LLM inference, we propose speculative actions, alossless framework for general agentic systems that predicts likely actionsusing faster models, enabling multiple steps to be executed in parallel.

50, TITLE: Multilingual Routing in Mixture-of-Experts
AUTHORS: Lucas Bandarkar; Chenyuan Yang; Mohsen Fayyaz; Junlin Hu; Nanyun Peng
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: In this work, we analyze expert routing patterns usingparallel multilingual datasets and present highly interpretable layer-wisephenomena.

51, TITLE: Decoupling Task-Solving and Output Formatting in LLM Generation
AUTHORS: Haikang Deng; Po-Nien Kung; Nanyun Peng
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: Thisdifficulty is especially common when instructive prompts intertwine reasoningdirectives -- specifying what the model should solve -- with rigid formattingrequirements that dictate how the solution must be presented.

52, TITLE: BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs
AUTHORS: Ivo Petrov; Jasper Dekoninck; Martin Vechev
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: However, existingbenchmarks that measure sycophancy in mathematics are limited: they focussolely on final-answer problems, rely on very simple and often contaminateddatasets, and construct benchmark samples using synthetic modifications thatcreate ill-posed questions rather than well-posed questions that aredemonstrably false. To address these issues, we introduce BrokenMath, the firstbenchmark for evaluating sycophantic behavior in LLMs within the context ofnatural language theorem proving.

53, TITLE: GuidedSampling: Steering LLMs Towards Diverse Candidate Solutions at Inference-Time
AUTHORS: Divij Handa; Mihir Parmar; Aswin RRV; Md Nayem Uddin; Hamid Palangi; Chitta Baral
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: To address thislimitation, we propose a new inference algorithm, GuidedSampling, whichdecouples the exploration and generation phases during inference, increasingdiversity of generated candidate solutions.

54, TITLE: Enhancing Fake News Video Detection Via LLM-Driven Creative Process Simulation
AUTHORS: Yuyan Bu; Qiang Sheng; Juan Cao; Shaofei Wang; Peng Qi; Yuhui Shi; Beizhe Hu
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: To address thisissue, we propose a data augmentation framework, AgentAug, that generatesdiverse fake news videos by simulating typical creative processes.

55, TITLE: MLLMEraser: Achieving Test-Time Unlearning in Multimodal Large Language Models Through Activation Steering
AUTHORS: Chenlu Ding; Jiancan Wu; Leheng Sheng; Fan Zhang; Yancheng Yuan; Xiang Wang; Xiangnan He
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: In this work, we propose MLLMEraser, aninput-aware, training-free framework for test-time unlearning.

56, TITLE: Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert
AUTHORS: Mingyu Liu; Zheng Huang; Xiaoyi Lin; Muzhi Zhu; Canyu Zhao; Zongze Du; Yating Wang; Haoyi Zhu; Hao Chen; Chunhua Shen
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: To address theselimitations, we introduce, for the first time, a framework centered around ageneralizable action expert.

57, TITLE: StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation
AUTHORS: Mingyu Liu; Jiuhe Shu; Hui Chen; Zeju Li; Canyu Zhao; Jiange Yang; Shenyuan Gao; Hao Chen; Chunhua Shen
CATEGORY: arxiv-cs.RO [RO]
HIGHLIGHT: A fundamental challenge in embodied intelligence is developing expressive andcompact state representations for efficient world modeling and decision making.However, existing methods often fail to achieve this balance, yieldingrepresentations that are either overly redundant or lacking in task-criticalinformation. We propose an unsupervised approach that learns a highlycompressed two-token state representation using a lightweight encoder and apre-trained Diffusion Transformer (DiT) decoder, capitalizing on its stronggenerative prior.

58, TITLE: NoTVLA: Narrowing of Dense Action Trajectories for Generalizable Robot Manipulation
AUTHORS: Zheng Huang; Mingyu Liu; Xiaoyi Lin; Muzhi Zhu; Canyu Zhao; Zongze Du; Xiaoman Li; Yiduo Jia; Hao Zhong; Hao Chen; Chunhua Shen
CATEGORY: arxiv-cs.RO [RO]
HIGHLIGHT: This issue stems from their overrelianceon continuous action sequences or action chunks, which inadvertently createisolated data silos that disrupt knowledge retention across tasks. To tacklethese challenges, we propose the Narrowing of Trajectory VLA (NoTVLA)framework: a novel approach that narrows its focus to sparse trajectories,thereby avoiding the catastrophic forgetting associated with dense trajectoryfine-tuning.

59, TITLE: Rare Text Semantics Were Always There in Your Diffusion Transformer
AUTHORS: Seil Kang; Woojung Han; Dayun Ju; Seong Jae Hwang
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: In this paper, we propose a simple yeteffective intervention that surfaces rare semantics inside MM-DiTs withoutadditional training steps, data, denoising-time optimization, or reliance onexternal modules (e.g., large language models).

60, TITLE: Epistemic Diversity and Knowledge Collapse in Large Language Models
AUTHORS: Dustin Wright; Sarah Masud; Jared Moore; Srishti Yadav; Maria Antoniak; Chan Young Park; Isabelle Augenstein
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: Existing works on homogenization are limited by a focus on closed-endedmultiple-choice setups or fuzzy semantic features, and do not look at trendsacross time and cultural contexts. To overcome this, we present a newmethodology to measure epistemic diversity, i.e., variation in real-worldclaims in LLM outputs, which we use to perform a broad empirical study of LLMknowledge collapse.

61, TITLE: Unmasking Puppeteers: Leveraging Biometric Leakage to Disarm Impersonation in AI-based Videoconferencing
AUTHORS: Danial Samadi Vahdati; Tai Duc Nguyen; Ekta Prashnani; Koki Nagano; David Luebke; Orazio Gallo; Matthew Stamm
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: To address this security problem, we exploit a keyobservation: the pose-expression latent inherently contains biometricinformation of the driving identity. Therefore, we introduce the firstbiometric leakage defense without ever looking at the reconstructed RGB video:a pose-conditioned, large-margin contrastive encoder that isolates persistentidentity cues inside the transmitted latent while cancelling transient pose andexpression.

62, TITLE: LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning
AUTHORS: Haoqiang Kang; Yizhe Zhang; Nikki Lijing Kuang; Nicklas Majamaki; Navdeep Jaitly; Yi-An Ma; Lianhui Qin
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: In thispaper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoningframework that unifies the expressiveness of continuous latent representationwith the iterative refinement capabilities of latent diffusion models for anexisting LLM.

63, TITLE: MARS: Optimizing Dual-System Deep Research Via Multi-Agent Reinforcement Learning
AUTHORS: Guoxin Chen; Zile Qiao; Wenqing Wang; Donglei Yu; Xuanzhong Chen; Hao Sun; Minpeng Liao; Kai Fan; Yong Jiang; Penguin Xie; Wayne Xin Zhao; Ruihua Song; Fei Huang
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: This paperintroduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamlessintegration of System 1's fast, intuitive thinking with System 2's deliberatereasoning within LLMs. MARS strategically integrates multiple external tools,such as Google Search, Google Scholar, and Python Interpreter, to accessup-to-date information and execute complex computations, while creating aspecialized division of labor where System 1 efficiently processes andsummarizes high-volume external information, providing distilled insights thatexpand System 2's reasoning context without overwhelming its capacity.Furthermore, we propose a multi-agent reinforcement learning frameworkextending Group Relative Policy Optimization to simultaneously optimize bothsystems with multi-turn tool interactions, bin-packing optimization, and samplebalancing strategies that enhance collaborative efficiency.

64, TITLE: What Makes Diffusion Language Models Super Data Learners?
AUTHORS: Zitian Gao; Haoming Luo; Lynx Chen; Jason Klein Liu; Ran Tao; Joey Zhou; Bryan Dai
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: Recent studies have shown that diffusion language models achieve remarkabledata efficiency under limited-data constraints, yet the underlying mechanismsremain unclear. In this work, we perform extensive ablation experiments todisentangle the sources of this efficiency.

65, TITLE: Pushing on Multilingual Reasoning Models with Language-Mixed Chain-of-Thought
AUTHORS: Guijin Son; Donghun Yang; Hitesh Laxmichand Patel; Amit Agarwal; Hyunwoo Ko; Chanuk Lim; Srikant Panda; Minhyuk Kim; Nikunj Drolia; Dasol Choi; Kyong-Ha Lee; Youngjae Yu
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: We train ninve models (4B-35B) across six families (Qwen2.5,Llama-3.1, Gemma-3, etc).

66, TITLE: Distilling Reasoning Into Student LLMs: Local Naturalness for Selecting Teacher Data
AUTHORS: Hoang Anh Just; Myeongseob Ko; Ruoxi Jia
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: We firstshow that the current method, which picks responses the student assigns thehighest global log-probability (global naturalness), fails when responses comefrom multiple teachers, i.e., global naturalness no longer correlates withdownstream performance, especially as the reasoning traces from strong teachersbecome longer. To overcome this problem, we introduce Local Naturalness, whichmeasures the student's log-probabilities over short, sequential reasoning stepsconditioned only on a small local window.

67, TITLE: SketchPlan: Diffusion Based Drone Planning From Human Sketches
AUTHORS: Sixten Norelius; Aaron O. Feldman; Mac Schwager
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: We propose SketchPlan, a diffusion-based planner that interprets 2Dhand-drawn sketches over depth images to generate 3D flight paths for dronenavigation.

68, TITLE: From Poisoned to Aware: Fostering Backdoor Self-Awareness in LLMs
AUTHORS: Guangyu Shen; Siyuan Cheng; Xiangzhe Xu; Yuan Zhou; Hanxi Guo; Zhuo Zhang; Xiangyu Zhang
CATEGORY: arxiv-cs.CR [CR]
HIGHLIGHT: Existing safety training methods largely fail to addressthis vulnerability, due to the inherent difficulty of uncovering hiddentriggers implanted in the model. Motivated by recent findings on LLMs'situational awareness, we propose a novel post-training framework thatcultivates self-awareness of backdoor risks and enables models to articulateimplanted triggers even when they are absent from the prompt.

69, TITLE: UGround: Towards Unified Visual Grounding with Unrolled Transformers
AUTHORS: Rui Qian; Xin Yin; Chuanhang Deng; Zhiyuan Peng; Jian Xiong; Wei Zhai; Dejing Dou
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: We present UGround, a \textbf{U}nified visual \textbf{Ground}ing paradigmthat dynamically selects intermediate layers across \textbf{U}nrolledtransformers as ``mask as prompt'', diverging from the prevailing pipeline thatleverages the fixed last hidden layer as ``\texttt{} as prompt''.

70, TITLE: Slm-mux: Orchestrating Small Language Models for Reasoning
AUTHORS: Chenyu Wang; Zishen Wan; Hao Kang; Emma Chen; Zhiqiang Xie; Tushar Krishna; Vijay Janapa Reddi; Yilun Du
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: Existing orchestration methods have primarily targetedfrontier models (e.g., GPT-4) and perform suboptimally when applied to SLMs. Toaddress this gap, we propose a three-stage approach for orchestrating SLMs.First, we introduce SLM-MUX, a multi-model architecture that effectivelycoordinates multiple SLMs.

71, TITLE: Flexible Locomotion Learning with Diffusion Model Predictive Control
AUTHORS: Runhan Huang; Haldun Balim; Heng Yang; Yilun Du
CATEGORY: arxiv-cs.RO [RO]
HIGHLIGHT: We present Diffusion-MPC, whichleverages a learned generative diffusion model as an approximate dynamics priorfor planning, enabling flexible test-time adaptation through reward andconstraint based optimization.

72, TITLE: LLM- Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and Rationale Inference in Imperfect Information Collaboration Game
AUTHORS: Fangzhou Liang; Tianshi Zheng; Chunkit Chan; Yauwai Yim; Yangqiu Song
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: This study introduces LLM-Hanabi, a novel benchmark that uses the cooperativegame Hanabi to evaluate the rationale inference and ToM of LLMs.

73, TITLE: WebRenderBench: Enhancing Web Interface Generation Through Layout-Style Consistency and Reinforcement Learning
AUTHORS: Peichao Lai; Jinhui Zhuang; Kexuan Zhang; Ningchang Xiong; Shengjie Wang; Yanwei Xu; Chong Chen; Yilei Wang; Bin Cui
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: Advances in multimodal largelanguage models (MLLMs) have made WebUI-to-Code increasingly feasible, yetexisting benchmarks remain limited in data diversity and evaluationreliability. To address these issues, we present WebRenderBench, a large-scalebenchmark of 22.5k webpages collected from real-world portal sites, offeringgreater diversity, complexity, and realism than prior benchmarks.

74, TITLE: Harnessing LLM for Noise-Robust Cognitive Diagnosis in Web-Based Intelligent Education Systems
AUTHORS: Guixian Zhang; Guan Yuan; Ziqi Xu; Yanmei Zhang; Jing Ren; Zhenyun Deng; Debo Cheng
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: Specially, WIES's open environmentcontinuously attracts new students and produces vast amounts of response logs,exacerbating the data imbalance and noise issues inherent in traditionaleducational systems. To address these challenges, we propose DLLM, aDiffusion-based LLM framework for noise-robust cognitive diagnosis.

75, TITLE: Aligning Perception, Reasoning, Modeling and Interaction: A Survey on Physical AI
AUTHORS: Kun Xiang; Terry Jingchen Zhang; Yinya Huang; Jixi He; Zirong Liu; Yueling Tang; Ruizhe Zhou; Lijing Luo; Youpeng Wen; Xiuwei Chen; Bingqian Lin; Jianhua Han; Hang Xu; Hanhui Li; Bin Dong; Xiaodan Liang
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: The rapid advancement of embodied intelligence and world models hasintensified efforts to integrate physical laws into AI systems, yet physicalperception and symbolic physics reasoning have developed along separatetrajectories without a unified bridging framework.

76, TITLE: MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition
AUTHORS: Umberto Cappellazzo; Minsu Kim; Pingchuan Ma; Honglie Chen; Xubo Liu; Stavros Petridis; Maja Pantic
CATEGORY: arxiv-eess.AS [AS]
HIGHLIGHT: Matryoshka representation learning(MRL) addresses this by enabling a single model to operate across multipletoken granularities, allowing compression rates to be adjusted dynamically.However, current MRL-based methods treat each scale independently duringtraining, limiting cross-scale generalization, robustness at high compression,and interpretability. To overcome these limitations, we propose MoME (Mixtureof Matryoshka Experts), a novel framework that integrates sparseMixture-of-Experts (MoE) into MRL-based LLMs for AVSR.

77, TITLE: TOPO-Bench: An Open-Source Topological Mapping Evaluation Framework with Quantifiable Perceptual Aliasing
AUTHORS: Jiaming Wang; Diwen Liu; Jizhuo Chen; Harold Soh
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: Weaddress these gaps by (1) formalizing topological consistency as thefundamental property of topological maps and showing that localization accuracyprovides an efficient and interpretable surrogate metric, and (2) proposing thefirst quantitative measure of dataset ambiguity to enable fair comparisonsacross environments. To support this protocol, we curate a diverse benchmarkdataset with calibrated ambiguity levels, implement and release deep-learnedbaseline systems, and evaluate them alongside classical methods.

78, TITLE: AlphaApollo: Orchestrating Foundation Models and Professional Tools Into A Self-Evolving System for Deep Agentic Reasoning
AUTHORS: Zhanke Zhou; Chentao Cao; Xiao Feng; Xuan Li; Zongze Li; Xiangyu Lu; Jiangchao Yao; Weikai Huang; Linrui Xu; Tian Cheng; Guanyu Jiang; Yiming Zheng; Brando Miranda; Tongliang Liu; Sanmi Koyejo; Masashi Sugiyama; Bo Han
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: We present AlphaApollo, a self-evolving agentic reasoning system that aims toaddress two bottlenecks in foundation model (FM) reasoning-limitedmodel-intrinsic capacity and unreliable test-time iteration.

79, TITLE: OptAgent: Optimizing Query Rewriting for E-commerce Via Multi-Agent Simulation
AUTHORS: Divij Handa; David Blincoe; Orson Adams; Yinlin Fu
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: While LLMs excel in verifiable tasks like coding and mathematics,where gold-standard solutions are available, adoption remains challenging forsubjective tasks that lack a single correct answer. E-commerce Query Rewriting(QR) is one such problem where determining whether a rewritten query properlycaptures the user intent is extremely difficult to figure out algorithmically.In this work, we introduce OptAgent, a novel framework that combinesmulti-agent simulations with genetic algorithms to verify and optimize queriesfor QR.

80, TITLE: Guided Query Refinement: Multimodal Hybrid Retrieval with Test-Time Optimization
AUTHORS: Omri Uzan; Asaf Yehudai; Roi pony; Eyal Shnarch; Ariel Gera
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: Furthermore, purely vision-centric approaches may beconstrained by the inherent modality gap still exhibited by modernvision-language models. In this work, we connect these challenges to theparadigm of hybrid retrieval, investigating whether a lightweight dense textretriever can enhance a stronger vision-centric model.

81, TITLE: GILT: An LLM-Free, Tuning-Free Graph Foundational Model for In-Context Learning
AUTHORS: Weishuo Ma; Yanbo Wang; Xiyuan Wang; Lei Zou; Muhan Zhang
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: In this work, we move beyond theselimitations and introduce \textbf{G}raph \textbf{I}n-context \textbf{L}earning\textbf{T}ransformer (GILT), a framework built on an LLM-free and tuning-freearchitecture.

82, TITLE: Vision Transformer for Transient Noise Classification
AUTHORS: Divyansh Srivastava; Andrzej Niedzielski
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: We aim to classify glitches in LIGO data into 22 existingclasses from the first run plus 2 additional noise classes from O3a using theVision Transformer (ViT) model.

83, TITLE: TS-Reasoner: Aligning Time Series Foundation Models with LLM Reasoning
AUTHORS: Fangxu Yu; Hongyu Zhao; Tianyi Zhou
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: Specifically, we propose a simple yet effective method to curatediverse, synthetic pairs of time series and textual captions for alignmenttraining.

84, TITLE: Replacing Softmax Similarity with A Sharpened Angular Similarity: Theory and Practice of Scaling To Billion-Context Attention
AUTHORS: Sahil Joshi; Agniva Chowdhury; Amar Kanakamedala; Ekam Singh; Evan Tu; Anshumali Shrivastava
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: Abstract: Softmax Attention has a quadratic time complexity, which becomes prohibitiveto run at long contexts, even with highly optimized GPU kernels. For example,FlashAttention (an exact, ...

85, TITLE: UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG
AUTHORS: Xiangyu Peng; Can Qin; Zeyuan Chen; Ran Xu; Caiming Xiong; Chien-Sheng Wu
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: In this paper, we introduceUniDoc-Bench, the first large-scale, realistic benchmark for MM-RAG built from70k real-world PDF pages across eight domains.

86, TITLE: LMM-Incentive: Large Multimodal Model-based Incentive Design for User-Generated Content in Web 3.0
AUTHORS: Jinbo Wen; Jiawen Kang; Linfeng Zhang; Xiaoying Tang; Jianhang Tang; Yang Zhang; Zhaohui Yang; Dusit Niyato
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: Specifically, we propose an LMM-basedcontract-theoretic model to motivate users to generate high-quality UGC,thereby mitigating the adverse selection problem from information asymmetry.

87, TITLE: LoRA Patching: Exposing The Fragility of Proactive Defenses Against Deepfakes
AUTHORS: Zuomin Qu; Yimao Guo; Qianyue Hu; Wei Lu
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: We propose a novel approach,Low-Rank Adaptation (LoRA) patching, which injects a plug-and-play LoRA patchinto Deepfake generators to bypass state-of-the-art defenses.

88, TITLE: What Scales in Cross-Entropy Scaling Law?
AUTHORS: Junxi Yan; Zixi Wei; Jingtao Zhan; Qingyao Ai; Yiqun Liu
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: However,recent evidence indicates that this law breaks down at very large scales: theloss decreases more slowly than expected, which causes significant trouble fordeveloping large language models. In this paper, we hypothesize that the rootcause lies in the fact that cross-entropy itself does not truly scale; instead,only one of its hidden components does.

89, TITLE: P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs
AUTHORS: Shuai Zhao; Xinyi Wu; Shiqian Zhao; Xiaobao Wu; Zhongliang Guo; Yanhao Jia; Anh Tuan Luu
CATEGORY: arxiv-cs.CR [CR]
HIGHLIGHT: Inthis study, we propose Poison-to-Poison (P2P), a general and effective backdoordefense algorithm.

90, TITLE: Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs
AUTHORS: Zishang Jiang; Jinyi Han; Tingyun Li; Xinyi Wang; Sihang Jiang; Jiaqing Liang; Zhaoqian Dai; Shuguang Ma; Fei Yu; Yanghua Xiao
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: To address this, we argue that the expert only needs to provideguidance only at critical decision points rather than the entire reasoningpath. Based on this insight, we propose MENTOR: Mixed-policy Expert Navigationfor Token-level Optimization of Reasoning, a framework that provides expertguidance only at critical decision points to perform effective and diverseexploration in RLVR.

91, TITLE: Beyond The Seen: Bounded Distribution Estimation for Open-Vocabulary Learning
AUTHORS: Xiaomeng Fan; Yuchuan Mao; Zhi Gao; Yuwei Wu; Jin Chen; Yunde Jia
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: We theoretically demonstrate that the distribution can be effectivelyestimated by generating unseen-class data, through which the estimation erroris upper-bounded. Building on this theoretical insight, we propose a novel open-vocabularylearning method, which generates unseen-class data for estimating thedistribution in open environments.

92, TITLE: Zephyrus: An Agentic Framework for Weather Science
AUTHORS: Sumanth Varambally; Marshall Fisher; Jas Thakker; Yiwei Chen; Zhirui Xia; Yasaman Jafari; Ruijia Niu; Manas Jain; Veeramakali Vignesh Manivannan; Zachary Novack; Luyu Han; Srikar Eranky; Salva Rühling Cachay; Taylor Berg-Kirkpatrick; Duncan Watson-Parris; Yi-An Ma; Rose Yu
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: Large languagemodels (LLMs) excel at understanding and generating text but cannot reasonabout high-dimensional meteorological datasets. We bridge this gap by buildinga novel agentic framework for weather science.

93, TITLE: Towards Policy-Compliant Agents: Learning Efficient Guardrails For Policy Violation Detection
AUTHORS: Xiaofei Wen; Wenjie Jacky Mo; Yanan Xie; Peng Qi; Muhao Chen
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: In addition to full-trajectory evaluation,PolicyGuardBench also includes a prefix-based violation detection task wheremodels must anticipate policy violations from truncated trajectory prefixesrather than complete sequences. Using this dataset, we train PolicyGuard-4B, alightweight guardrail model that delivers strong detection accuracy across alltasks while keeping inference efficient.

94, TITLE: MetaFind: Scene-Aware 3D Asset Retrieval for Coherent Metaverse Scene Generation
AUTHORS: Zhenyu Pan; Yucheng Lu; Han Liu
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: We present MetaFind, a scene-aware tri-modal compositional retrievalframework designed to enhance scene generation in the metaverse by retrieving3D assets from large-scale repositories.

95, TITLE: Toward A Unified Geometry Understanding: Riemannian Diffusion Framework for Graph Generation and Prediction
AUTHORS: Yisen Gao; Xingcheng Fu; Qingyun Sun; Jianxin Li; Xianxian Li
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: To address manifold deviation, we introduce amanifold-constrained diffusion method and a self-guided strategy forunconditional generation, ensuring that the generated data remains aligned withthe manifold signature.

96, TITLE: Online Automatic Code Generation for Robot Swarms: LLMs and Self-organizing Hierarchy
AUTHORS: Weixu Zhu; Marco Dorigo; Mary Katherine Heinrich
CATEGORY: arxiv-cs.RO [RO]
HIGHLIGHT: Our recently introduced self-organizing nervous system (SoNS) provides robotswarms with 1) ease of behavior design and 2) global estimation of the swarmconfiguration and its collective environment, facilitating the implementationof online automatic code generation for robot swarms. In a demonstration with 6real robots and simulation trials with >30 robots, we show that when aSoNS-enhanced robot swarm gets stuck, it can automatically solicit and run codegenerated by an external LLM on the fly, completing its mission with an 85%success rate.

97, TITLE: ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context
AUTHORS: Huiwon Jang; Sihyun Yu; Heeseung Kwon; Hojin Jeon; Younggyo Seo; Jinwoo Shin
CATEGORY: arxiv-cs.RO [RO]
HIGHLIGHT: In thispaper, we introduce ContextVLA, a policy model that robustly improves robotictask performance by effectively leveraging multi-frame observations.

98, TITLE: Video-LMM Post-Training: A Deep Dive Into Video Reasoning with Large Multimodal Models
AUTHORS: Yolo Yunlong Tang; Jing Bi; Pinxin Liu; Zhenyu Pan; Zhangyun Tan; Qianxiang Shen; Jiani Liu; Hang Hua; Junjia Guo; Yunzhong Xiao; Chao Huang; Zhiyuan Wang; Susan Liang; Xinyi Liu; Yizhi Song; Yuhe Nie; Jia-Xing Zhong; Bozheng Li; Daiqing Qi; Ziyun Zeng; Ali Vosoughi; Luchuan Song; Zeliang Zhang; Daiki Shimada; Han Liu; Jiebo Luo; Chenliang Xu
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: This survey aims to provideresearchers and practitioners with a unified framework for advancing Video-LMMcapabilities.

99, TITLE: Investigating LLM Variability in Personalized Conversational Information Retrieval
AUTHORS: Simon Lupart; Daniël van Dijk; Eric Langezaal; Ian van Dort; Mohammad Aliannejadi
CATEGORY: arxiv-cs.IR [IR]
HIGHLIGHT: In thisreproducibility study, we rigorously reproduce and extend their work, focusingon LLM output variability and model generalization.

100, TITLE: VaseVQA-3D: Benchmarking 3D VLMs on Ancient Greek Pottery
AUTHORS: Nonghai Zhang; Zeyu Zhang; Jiazi Wang; Yang Zhao; Hao Tang
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: However, whendealing with specialized cultural heritage domains like 3D vase artifacts,existing models face severe data scarcity issues and insufficient domainknowledge limitations. Due to the lack of targeted training data, current VLMsstruggle to effectively handle such culturally significant specialized tasks.To address these challenges, we propose the VaseVQA-3D dataset, which serves asthe first 3D visual question answering dataset for ancient Greek potteryanalysis, collecting 664 ancient Greek vase 3D models with correspondingquestion-answer data and establishing a complete data construction pipeline.

101, TITLE: Optimized Minimal 4D Gaussian Splatting
AUTHORS: Minseo Lee; Byeonghyeon Lee; Lucas Yunkyu Lee; Eunsoo Lee; Sangmin Kim; Seunghyeon Song; Joo Chan Lee; Jong Hwan Ko; Jaesik Park; Eunbyung Park
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: In this work, we present OMG4 (OptimizedMinimal 4D Gaussian Splatting), a framework that constructs a compact set ofsalient Gaussians capable of faithfully representing 4D Gaussian models.

102, TITLE: FaithCoT-Bench: Benchmarking Instance-Level Faithfulness of Chain-of-Thought Reasoning
AUTHORS: Xu Shen; Song Wang; Zhen Tan; Laura Yao; Xinyu Zhao; Kaidi Xu; Xin Wang; Tianlong Chen
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: Large language models (LLMs) increasingly rely on Chain-of-Thought (CoT)prompting to improve problem-solving and provide seemingly transparentexplanations.

103, TITLE: Bridging The Gap Between Multimodal Foundation Models and World Models
AUTHORS: Xuehai He
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: Our approaches incorporate scene graphs, multimodal conditioning,and multimodal alignment strategies to guide the generation process, ensuringconsistency with high-level semantics and fine-grained user intent.

104, TITLE: What Is A Protest Anyway? Codebook Conceptualization Is Still A First-order Concern in LLM-era Classification
AUTHORS: Andrew Halterman; Katherine A. Keith
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: In this work, focus onthe steps before and after LLM prompting -- conceptualization of concepts to beclassified and using LLM predictions in downstream statistical inference --which we argue have been overlooked in much of LLM-era CSS.

105, TITLE: The Overlooked Value of Test-time Reference Sets in Visual Place Recognition
AUTHORS: Mubariz Zaffar; Liangliang Nan; Sebastian Scherer; Julian F. P. Kooij
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: Therefore,we propose to perform simple Reference-Set-Finetuning (RSF) of VPR models onthe map, boosting the SOTA (~2.3% increase on average for Recall@1) on thesechallenging datasets.

106, TITLE: Do LLMs Align with My Task? Evaluating Text-to-SQL Via Dataset Alignment
AUTHORS: Davood Rafiei; Morgan Lindsay Heisler; Weiwei Zhang; Mohammadreza Pourreza; Yong Zhang
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: Supervised Fine-Tuning (SFT) is an effective method for adapting LargeLanguage Models (LLMs) on downstream tasks.

107, TITLE: On The Limitations and Capabilities of Position Embeddings for Length Generalization
AUTHORS: Yang Chen; Yitao Liang; Zhouchen Lin
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: To enhance LG, we introduce Scale Hint, allowing flexible instancescaling, and a Learning-Based Position Embedding framework that automaticallylearns positional relations.

108, TITLE: The Hidden Game Problem
AUTHORS: Gon Buzaglo; Noah Golowich; Elad Hazan
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: We introduce thehidden game problem, where for each player, an unknown subset of strategiesconsistently yields higher rewards compared to the rest.

109, TITLE: Beyond Next-Token Prediction: A Performance Characterization of Diffusion Versus Autoregressive Language Models
AUTHORS: Minseo Kim; Coleman Hooper; Aditya Tomar; Chenfeng Xu; Mehrdad Farajtabar; Michael W. Mahoney; Kurt Keutzer; Amir Gholami
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: However, the performance implications ofDLMs relative to commonly deployed ARMs are not fully understood. In this work,we present a comprehensive performance study analyzing the performancecharacteristics of ARMs and DLMs, using both theoretical analysis and profilingdata to characterize the trade-offs between these approaches.

110, TITLE: Artificial-Intelligence Grading Assistance for Handwritten Components of A Calculus Exam
AUTHORS: Gerd Kortemeyer; Alexander Caspar; Daria Horica
CATEGORY: arxiv-cs.CY [CY]
HIGHLIGHT: We investigate whether contemporary multimodal LLMs can assist with gradingopen-ended calculus at scale without eroding validity.

111, TITLE: Quality‐Diversity Methods for The Modern Data Scientist
AUTHORS: Michiel Stock; Daan Van Hauwermeiren; Bernard De Baets; Steff Taelman; Dries Marzougui; Maxime Van Haeverbeke
CATEGORY: WIREs Computational Statistics [WIRES COMPUTATIONAL STATISTICS]
HIGHLIGHT: ABSTRACT Unlike gradient‐based methods, evolutionary algorithms use populations and exploit randomness to find novel and performant solutions.

112, TITLE: Reliable and Scalable Robot Policy Evaluation with Imperfect Simulators
AUTHORS: Apurva Badithela; David Snyder; Lihan Zha; Joseph Mikhail; Matthew O'Kelly; Anushri Dixit; Anirudha Majumdar
CATEGORY: arxiv-cs.RO [RO]
HIGHLIGHT: Our key idea is to formalize the problem of combiningreal and simulation evaluations as a prediction-powered inference problem, inwhich a small number of paired real and simulation evaluations are used torectify bias in large-scale simulation.

113, TITLE: EvoEngineer: Mastering Automated CUDA Kernel Code Evolution with Large Language Models
AUTHORS: Ping Guo; Chenyu Zhu; Siyuan Chen; Fei Liu; Xi Lin; Zhichao Lu; Qingfu Zhang
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: Furthermore, general-purpose LLM code evolution methods cannot meet strictcorrectness requirements of CUDA kernel optimization. We address these fundamental challenges by first formalizing CUDA kerneloptimization as a code optimization task with a clear objective, constraints,and evaluation metrics.

114, TITLE: Know Thyself? On The Incapability and Implications of AI Self-Recognition
AUTHORS: Xiaoyan Bai; Aryan Shrivastava; Ari Holtzman; Chenhao Tan
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: Motivated by contradictory interpretations of whethermodels possess self-recognition (Panickssery et al., 2024; Davidson et al.,2024), we introduce a systematic evaluation framework that can be easilyapplied and updated.

115, TITLE: VLPRSDet: A Vision–language Pretrained Model for Remote Sensing Object Detection
AUTHORS: Dongyang Liu; Xuejian Liang; Yunxiao Qi; Yunqiao Xi; Jing Jin; Junping Zhang
CATEGORY: Neurocomputing [NEUROCOMPUTING]
HIGHLIGHT: 

116, TITLE: ContraGen: A Multi-Agent Generation Framework for Enterprise Contradictions Detection
AUTHORS: Ananya Mantravadi; Shivali Dalmia; Abhishek Mukherji; Nand Dave; Anudha Mittal
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: To address thislimitation, we propose ContraGen, a contradiction-aware benchmark frameworktailored to enterprise domain.

117, TITLE: SegMASt3R: Geometry Grounded Segment Matching
AUTHORS: Rohit Jayanti; Swayam Agrawal; Vansh Garg; Siddharth Tourani; Muhammad Haris Khan; Sourav Garg; Madhava Krishna
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: In thispaper, we leverage the spatial understanding of 3D foundation models to tacklewide-baseline segment matching, a challenging setting involving extremeviewpoint shifts. We propose an architecture that uses the inductive bias ofthese 3D foundation models to match segments across image pairs with up to 180degree view-point change.

118, TITLE: Operationalizing Data Minimization for Privacy-Preserving LLM Prompting
AUTHORS: Jijie Zhou; Niloofar Mireshghallah; Tianshi Li
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: To obtain usefulresponses, users often share more than necessary, increasing privacy risks viamemorization, context-based personalization, or security breaches. We present aframework to formally define and operationalize data minimization: for a givenuser prompt and response model, quantifying the least privacy-revealingdisclosure that maintains utility, and we propose a priority-queue tree searchto locate this optimal point within a privacy-ordered transformation space.

119, TITLE: Representation Potentials of Foundation Models for Multimodal Alignment: A Survey
AUTHORS: Jianglin Lu; Hailing Wang; Yi Xu; Yizhou Wang; Kuo Yang; Yun Fu
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: In this survey, we investigate therepresentation potentials of foundation models, defined as the latent capacityof their learned representations to capture task-specific information within asingle modality while also providing a transferable basis for alignment andunification across modalities.

120, TITLE: Learning Efficient Meshflow and Optical Flow from Event Cameras
AUTHORS: Xinglong Luo; Ao Luo; Kunming Luo; Zhengning Wang; Ping Tan; Bing Zeng; Shuaicheng Liu
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: In this paper, we explore the problem of event-based meshflow estimation, anovel task that involves predicting a spatially smooth sparse motion field fromevent cameras.

121, TITLE: TiTok: Transfer Token-level Knowledge Via Contrastive Excess to Transplant LoRA
AUTHORS: Chanjoo Jung; Jaehyung Kim
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: In this paper, we propose TiTok, a newframework that enables effective LoRA Transplantation through Token-levelknowledge transfer.

122, TITLE: Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization
AUTHORS: Jiaxin Deng; Junbiao Pang
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: Abstract: Sharpness-Aware Minimization (SAM) improves model generalization but doublesthe computational cost of Stochastic Gradient Descent (SGD) by requiring twicethe gradient calculations ...

123, TITLE: Implicit Models: Expressive Power Scales with Test-Time Compute
AUTHORS: Jialin Liu; Lisang Ding; Stanley Osher; Wotao Yin
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: While it is empirically known that these compact models canoften match or even exceed larger explicit networks by allocating moretest-time compute, the underlying mechanism remains poorly understood. We study this gap through a nonparametric analysis of expressive power.

124, TITLE: FT-MDT: Extracting Decision Trees from Medical Texts Via A Novel Low-rank Adaptation Method
AUTHORS: Yuheng Li; Jiechao Gao; Wei Han; Wenwen Ouyang; Wei Zhu; Hui Yi Leong
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: Abstract: Knowledge of the medical decision process, which can be modeled as medicaldecision trees (MDTs), is critical to building clinical decision supportsystems. However, current MDT ...

125, TITLE: SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs
AUTHORS: Dachuan Shi; Abedelkadir Asi; Keying Li; Xiangchi Yuan; Leyan Pan; Wenke Lee; Wen Xiao
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: Recent work shows that, beyond discrete reasoning through explicitchain-of-thought steps, which are limited by the boundaries of naturallanguages, large language models (LLMs) can also reason continuously in latentspace, allowing richer information per step and thereby improving tokenefficiency. Despite this promise, latent reasoning still faces two challenges,especially in training-free settings: 1) purely latent reasoning broadens thesearch distribution by maintaining multiple implicit paths, which diffusesprobability mass, introduces noise, and impedes convergence to a singlehigh-confidence solution, thereby hurting accuracy; and 2) overthinkingpersists even without explicit text, wasting tokens and degrading efficiency.To address these issues, we introduce SwiReasoning, a training-free frameworkfor LLM reasoning which features two key innovations: 1) SwiReasoningdynamically switches between explicit and latent reasoning, guided byblock-wise confidence estimated from entropy trends in next-tokendistributions, to balance exploration and exploitation and promote timelyconvergence.

126, TITLE: Provable Speech Attributes Conversion Via Latent Independence
AUTHORS: Jonathan Svirsky; Ofir Lindenbaum; Uri Shaham
CATEGORY: arxiv-cs.SD [SD]
HIGHLIGHT: In this work, we propose ageneral framework for speech attribute conversion, accompanied by theoreticalanalysis and guarantees under reasonable assumptions.

127, TITLE: Concept-Based Masking: A Patch-Agnostic Defense Against Adversarial Patch Attacks
AUTHORS: Ayushi Mehrotra; Derek Peng; Dipkamal Bhusal; Nidhi Rastogi
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: In thiswork, we propose a patch-agnostic defense that leverages concept-basedexplanations to identify and suppress the most influential concept activationvectors, thereby neutralizing patch effects without explicit detection.Evaluated on Imagenette with a ResNet-50, our method achieves higher robust andclean accuracy than the state-of-the-art PatchCleanser, while maintainingstrong performance across varying patch sizes and locations.

128, TITLE: LLM Ethics Benchmark: A Three-dimensional Assessment System for Evaluating Moral Reasoning in Large Language Models
AUTHORS: Junfeng Jiao; Saleh Afroogh; Abhejay Murali; Kevin Chen; David Atkinson; Amit Dhurandhar
CATEGORY: Scientific Reports [SCIENTIFIC REPORTS]
HIGHLIGHT: 

129, TITLE: Logistic-Gated Operators Enable Auditable Unit-Aware Thresholds in Symbolic Regression
AUTHORS: Ou Deng; Ruichen Cong; Jianting Xu; Shoji Nishimura; Atsushi Ogihara; Qun Jin
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: We propose logistic-gatedoperators (LGO) -- differentiable gates with learnable location and steepness-- embedded as typed primitives and mapped back to physical units for audit.Across two primary health datasets (ICU, NHANES), the hard-gate variantrecovers clinically plausible cut-points: 71% (5/7) of assessed thresholds fallwithin 10% of guideline anchors and 100% within 20%, while using far fewergates than the soft variant (ICU median 4.0 vs 10.0; NHANES 5.0 vs 12.5), andremaining within the competitive accuracy envelope of strong SR baselines.

130, TITLE: OptiFLIDS: Optimized Federated Learning for Energy-Efficient Intrusion Detection in IoT
AUTHORS: Saida Elouardi; Mohammed Jouhari; Anas Motii
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: Despite its advantages,FL still faces key challenges, such as data heterogeneity (non-IID data) andhigh energy and computation costs, particularly for resource constrained IoTdevices. To address these issues, this paper proposes OptiFLIDS, a novelapproach that applies pruning techniques during local training to reduce modelcomplexity and energy consumption.

131, TITLE: Studying The Korean Word-Chain Game with RLVR: Mitigating Reward Conflicts Via Curriculum Learning
AUTHORS: Donghwan Rho
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: In this work, we study theKorean word-chain game using RLVR.

132, TITLE: Unveiling LLMs' Metaphorical Understanding: Exploring Conceptual Irrelevance, Context Leveraging and Syntactic Influence
AUTHORS: Fengying Ye; Shanshan Wang; Lidia S. Chao; Derek F. Wong
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: This study examines LLMs' metaphor-processing abilities from threeperspectives: (1) Concept Mapping: using embedding space projections toevaluate how LLMs map concepts in target domains (e.g., misinterpreting "fallin love" as "drop down from love"); (2) Metaphor-Literal Repository: analyzingmetaphorical words and their literal counterparts to identify inherentmetaphorical knowledge; and (3) Syntactic Sensitivity: assessing howmetaphorical syntactic structures influence LLMs' performance.

133, TITLE: Thermal Comfort Assessment Based on Occupants’ Action Recognition Using Computer Vision - A Review
AUTHORS: Zhennan Wu; Zu Wang; Zhichen Wei; John Calautit
CATEGORY: Building and Environment [BUILDING AND ENVIRONMENT]
HIGHLIGHT: 

134, TITLE: Making Mathematical Reasoning Adaptive
AUTHORS: Zhejian Lai; Xiang Geng; Zhijun Wang; Yang Bai; Jiahuan Li; Rongxiang Weng; Jingang Wang; Xuezhi Cao; Xunliang Cai; Shujian Huang
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: This paper attributes these deficiencies to spurious reasoning,i.e., producing answers from superficial features. To address this challenge,we propose the AdaR framework to enable adaptive reasoning, wherein models relyon problem-solving logic to produce answers.

135, TITLE: SpikingMamba: Towards Energy-Efficient Large Language Models Via Knowledge Distillation from Mamba
AUTHORS: Yulong Huang; Jianxiong Tang; Chao Wang; Ziyi Wang; Jianguo Zhang; Zhichao Lu; Bojun Cheng; Luziwei Leng
CATEGORY: arxiv-cs.NE [NE]
HIGHLIGHT: Large Language Models (LLMs) have achieved remarkable performance acrosstasks but remain energy-intensive due to dense matrix operations.

136, TITLE: Dual-stage and Lightweight Patient Chart Summarization for Emergency Physicians
AUTHORS: Jiajun Wu; Swaleh Zaidi; Braden Teitge; Henry Leung; Jiayu Zhou; Jessalyn Holodinsky; Steve Drew
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: We present a two-stage summarization system that runs entirely onembedded devices, enabling offline clinical summarization while preservingpatient privacy.

137, TITLE: Plug-and-Play Dramaturge: A Divide-and-Conquer Approach for Iterative Narrative Script Refinement Via Collaborative LLM Agents
AUTHORS: Wenda Xie; Chao Guo; Yanqing Jing. Junle Wang; Yisheng Lv; Fei-Yue Wang
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: To address theseissues, we propose Dramaturge, a task and feature oriented divide-and-conquerapproach powered by hierarchical multiple LLM agents.

138, TITLE: LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for Workflow Automation
AUTHORS: Dongge Han; Camille Couturier; Daniel Madrigal Diaz; Xuchao Zhang; Victor Rühle; Saravan Rajmohan
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: We introduce LEGOMem, a modular procedural memory framework for multi-agentlarge language model (LLM) systems in workflow automation.

139, TITLE: Probing Geometry of Next Token Prediction Using Cumulant Expansion of The Softmax Entropy
AUTHORS: Karthik Viswanathan; Sang Eon Park
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: We introduce a cumulant-expansion framework for quantifying how largelanguage models (LLMs) internalize higher-order statistical structure duringnext-token prediction.

140, TITLE: Embracing Discrete Search: A Reasonable Approach to Causal Structure Learning
AUTHORS: Marcel Wienöbst; Leonard Henckel; Sebastian Weichwald
CATEGORY: arxiv-stat.ML [ML]
HIGHLIGHT: We present FLOP (Fast Learning of Order and Parents), a score-based causaldiscovery algorithm for linear models.

141, TITLE: AURA Score: A Metric For Holistic Audio Question Answering Evaluation
AUTHORS: Satvik Dixit; Soham Deshmukh; Bhiksha Raj
CATEGORY: arxiv-eess.AS [AS]
HIGHLIGHT: To address the gap inliterature, we make three contributions in this work.

142, TITLE: PsycholexTherapy: Simulating Reasoning in Psychotherapy with Small Language Models in Persian
AUTHORS: Mohammad Amin Abbasi; Hassan Naderi
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: This study presents PsychoLexTherapy, a framework for simulatingpsychotherapeutic reasoning in Persian using small language models (SLMs).

143, TITLE: Detecting Invariant Manifolds in ReLU-Based RNNs
AUTHORS: Lukas Eisenmann; Alena Brändle; Zahra Monfared; Daniel Durstewitz
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: We demonstrate how the algorithm can be used totrace the boundaries between different basins of attraction, and hence tocharacterize multistability, a computationally important property.

144, TITLE: DHQA-4D: Perceptual Quality Assessment of Dynamic 4D Digital Human
AUTHORS: Yunhao Li; Sijing Wu; Yucheng Zhu; Huiyu Duan; Zicheng Zhang; Guangtao Zhai
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: Equippedwith DHQA-4D dataset, we analyze the influence of different types of distortionon human perception for textured dynamic 4D meshes and non-textured dynamic 4Dmeshes.

145, TITLE: Exploring Instruction Data Quality for Explainable Image Quality Assessment
AUTHORS: Yunhao Li; Sijing Wu; Huiyu Duan; Yucheng Zhu; Qi Jia; Guangtao Zhai
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: Beyond randomlysampling a subset, we propose a clustering-based data selection framework withthree stages: clustering feature extraction, cluster quota allocation, andcluster sampling strategy.

146, TITLE: Person-Centric Annotations of LAION-400M: Auditing Bias and Its Transfer to Models
AUTHORS: Leander Girrbach; Stephan Alaniz; Genevieve Smith; Trevor Darrell; Zeynep Akata
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: Vision-language models trained on large-scale multimodal datasets show strongdemographic biases, but the role of training data in producing these biasesremains unclear.

147, TITLE: Autonomy Matters: A Study on Personalization-Privacy Dilemma in LLM Agents
AUTHORS: Zhiping Zhang; Yi Evie Zhang; Freda Shi; Tianshi Li
CATEGORY: arxiv-cs.HC [HC]
HIGHLIGHT: We find that personalization withoutconsidering users' privacy preferences increases privacy concerns and decreasestrust and willingness to use.

148, TITLE: Token Hidden Reward: Steering Exploration-Exploitation in Group Relative Deep Reinforcement Learning
AUTHORS: Wenlong Deng; Yi Ren; Yushu Li; Boying Gong; Danica J. Sutherland; Xiaoxiao Li; Christos Thrampoulidis
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: We find that training dynamics are dominated by asmall subset of tokens with high absolute THR values.

149, TITLE: PhaseFormer: From Patches to Phases for Efficient and Effective Time Series Forecasting
AUTHORS: Yiming Niu; Jinliang Deng; Yongxin Tong
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: Thispaper provides, for the first time, a clear explanation of why patch-levelprocessing is inherently inefficient, supported by strong evidence fromreal-world data. To address these limitations, we introduce a phase perspectivefor modeling periodicity and present an efficient yet effective solution,PhaseFormer.

150, TITLE: Bridging Reasoning to Learning: Unmasking Illusions Using Complexity Out of Distribution Generalization
AUTHORS: Mohammad Mahdi Samiei Paqaleh; Arash Marioriyad; Arman Tahmasebi-Zadeh; Mohamadreza Fereydooni; Mahdi Ghaznavai; Mahdieh Soleymani Baghshah
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: We propose ComplexityOut of Distribution (Complexity OoD) generalization as a framework and problemsetting to define and measure reasoning.

151, TITLE: Can LLMs Detect Ambiguous Plural Reference? An Analysis of Split-Antecedent and Mereological Reference
AUTHORS: Dang Anh; Rick Nouwen; Massimo Poesio
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: Our goal is to study how LLMs represent and interpret plural reference inambiguous and unambiguous contexts.

152, TITLE: AvatarVTON: 4D Virtual Try-On for Animatable Avatars
AUTHORS: Zicheng Jiang; Jixin Gao; Shengfeng He; Xinzhe Li; Yulong Zheng; Zhaotong Yang; Junyu Dong; Yong Du
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: We propose AvatarVTON, the first 4D virtual try-on framework that generatesrealistic try-on results from a single in-shop garment image, enabling freepose control, novel-view rendering, and diverse garment choices.

153, TITLE: Real-Time Threaded Houbara Detection and Segmentation for Wildlife Conservation Using Mobile Platforms
AUTHORS: Lyes Saad Saoud; Loic Lesobre; Enrico Sorato; Irfan Hussain
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: We introduce acurated Houbara dataset of 40,000 annotated images to support model trainingand evaluation across diverse conditions.

154, TITLE: SSDD: Single-Step Diffusion Decoder for Efficient Image Tokenization
AUTHORS: Théophane Vallaeys; Jakob Verbeek; Matthieu Cord
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: To address theselimitations, we introduce a new pixel diffusion decoder architecture forimproved scaling and training stability, benefiting from transformer componentsand GAN-free training.

155, TITLE: AgentBuilder: Exploring Scaffolds for Prototyping User Experiences of Interface Agents
AUTHORS: Jenny T. Liang; Titus Barik; Jeffrey Nichols; Eldon Schoop; Ruijia Cheng
CATEGORY: arxiv-cs.HC [HC]
HIGHLIGHT: In this work, we explore the affordances agentprototyping systems should offer by conducting a requirements elicitation studywith 12 participants with varying experience with agents.

156, TITLE: Bio-Inspired Robotic Houbara: From Development to Field Deployment for Behavioral Studies
AUTHORS: Lyes Saad Saoud; Irfan Hussain
CATEGORY: arxiv-cs.RO [RO]
HIGHLIGHT: Field trials in desert aviaries demonstrated reliable real timeoperation at 15 to 22 FPS with latency under 100 ms and confirmed that theplatform elicits natural recognition and interactive responses from liveHoubara bustards under harsh outdoor conditions.

157, TITLE: Pulp Motion: Framing-aware Multimodal Camera and Human Motion Generation
AUTHORS: Robin Courant; Xi Wang; David Loiseaux; Marc Christie; Vicky Kalogeiton
CATEGORY: arxiv-cs.GR [GR]
HIGHLIGHT: We propose a simple,model-agnostic framework that enforces multimodal coherence via an auxiliarymodality: the on-screen framing induced by projecting human joints onto thecamera.

158, TITLE: Spatiotemporal Forecasting As Planning: A Model-Based Reinforcement Learning Approach with Generative World Models
AUTHORS: Hao Wu; Yuan Gao; Xingjian Shi; Shuaipeng Li; Fan Xu; Fan Zhang; Zhihong Zhu; Weiyan Wang; Xiao Luo; Kun Wang; Xian Wu; Xiaomeng Huang
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: Abstract: To address the dual challenges of inherent stochasticity andnon-differentiable metrics in physical spatiotemporal forecasting, we proposeSpatiotemporal Forecasting as Planning ...

159, TITLE: NEXUS: Network Exploration for EXploiting Unsafe Sequences in Multi-Turn LLM Jailbreaks
AUTHORS: Javad Rafiei Asl; Sidhant Narula; Mohammad Ghasemigol; Eduardo Blanco; Daniel Takabi
CATEGORY: arxiv-cs.CR [CR]
HIGHLIGHT: Abstract: Large Language Models (LLMs) have revolutionized natural language processingbut remain vulnerable to jailbreak attacks, especially multi-turn jailbreaksthat distribute malicious ...

160, TITLE: RAP: 3D Rasterization Augmented End-to-End Planning
AUTHORS: Lan Feng; Yang Gao; Eloi Zablocki; Quanyi Li; Wuyang Li; Sichao Liu; Matthieu Cord; Alexandre Alahi
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: In this work, we argue thatphotorealism is unnecessary for training end-to-end planners.

161, TITLE: Self Speculative Decoding for Diffusion Large Language Models
AUTHORS: Yifeng Gao; Ziang Ji; Yuxuan Wang; Biqing Qi; Hanlin Xu; Linfeng Zhang
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: However, thegeneration results of current parallel decoding methods deviate from stepwisedecoding, introducing potential performance degradation, which limits theirpractical deployment. To address this problem, we propose \textbf{S}elf\textbf{S}peculative \textbf{D}ecoding (SSD), a lossless inference accelerationmethod that leverages the dLLM itself as both speculative decoding drafter andverifier without auxiliary modules.

162, TITLE: MASC: Boosting Autoregressive Image Generation with A Manifold-Aligned Semantic Clustering
AUTHORS: Lixuan He; Shikang Zheng; Linfeng Zhang
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: To resolvethis, we propose Manifold-Aligned Semantic Clustering (MASC), a principledframework that constructs a hierarchical semantic tree directly from thecodebook's intrinsic structure.

163, TITLE: Let Features Decide Their Own Solvers: Hybrid Feature Caching for Diffusion Transformers
AUTHORS: Shikang Zheng; Guantao Chen; Qinming Zhou; Yuqi Lin; Lixuan He; Chang Zou; Peiliang Cai; Jiacheng Liu; Linfeng Zhang
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: However, existing methodsoften apply a uniform caching strategy across all feature dimensions, ignoringtheir heterogeneous dynamic behaviors. Therefore, we adopt a new perspective bymodeling hidden feature evolution as a mixture of ODEs across dimensions, andintroduce HyCa, a Hybrid ODE solver inspired caching framework that appliesdimension-wise caching strategies.

164, TITLE: CCD-Bench: Probing Cultural Conflict in Large Language Model Decision-Making
AUTHORS: Hasibur Rahman; Hanan Salam
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: Existing benchmarks predominantly target cultural knowledge(CulturalBench), value prediction (WorldValuesBench), or single-axis biasdiagnostics (CDEval); none evaluate how LLMs adjudicate when multipleculturally grounded values directly clash. We address this gap with CCD-Bench,a benchmark that assesses LLM decision-making under cross-cultural valueconflict.

165, TITLE: Contrastive Learning Using Graph Embeddings for Domain Adaptation of Language Models in The Process Industry
AUTHORS: Anastasia Zhukova; Jonas Lührs; Christian E. Lobmüller; Bela Gipp
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: Recent trends in NLP utilize knowledge graphs (KGs) to enhance pretrainedlanguage models by incorporating additional knowledge from the graph structuresto learn domain-specific terminology or relationships between documents thatmight otherwise be overlooked.

166, TITLE: FrameOracle: Learning What to See and How Much to See in Videos
AUTHORS: Chaoyu Li; Tianzhi Li; Fei Tao; Zhenyu Zhao; Ziqian Wu; Maozheng Zhao; Juntong Song; Cheng Niu; Pooyan Fazli
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: Vision-language models (VLMs) have advanced video understanding, but theirperformance is limited by the number of input frames they can process.

167, TITLE: Physics-Inspired All-Pair Interaction Learning for 3D Dynamics Modeling
AUTHORS: Kai Yang; Yuqi Huang; Junheng Tao; Wanyu Wang; Qitian Wu
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: In this paper, we propose PAINET, a principledSE(3)-equivariant neural architecture for learning all-pair interactions inmulti-body systems.

168, TITLE: Deep Reinforcement Learning for Multi-Agent Coordination
AUTHORS: Kehinde O. Aina; Sehoon Ha
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: Drawing inspiration from insect colonies, whichachieve robust coordination through stigmergy -- modifying and interpretingenvironmental traces -- we propose a Stigmergic Multi-Agent Deep ReinforcementLearning (S-MADRL) framework that leverages virtual pheromones to model localand social interactions, enabling decentralized emergent coordination withoutexplicit communication.

169, TITLE: Improving Consistency in Retrieval-Augmented Systems with Group Similarity Rewards
AUTHORS: Faisal Hamman; Chenyang Zhu; Anoop Kumar; Xujun Peng; Sanghamitra Dutta; Daben Liu; Alfy Samuel
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: In this work, we focus on information consistency, i.e., therequirement that outputs convey the same core content across semanticallyequivalent inputs.

170, TITLE: Aria: An Agent For Retrieval and Iterative Auto-Formalization Via Dependency Graph
AUTHORS: Hanyu Wang; Ruohan Xie; Yutong Wang; Guoxiong Gao; Xintao Yu; Bin Dong
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: To ensure semantic correctness, we introduce AriaScorer, a checkerthat retrieves definitions from Mathlib for term-level grounding, enablingrigorous and reliable verification.

171, TITLE: Sample, Align, Synthesize: Graph-Based Response Synthesis with ConGrs
AUTHORS: Sayan Ghosh; Shahzaib Saqib Warraich; Dhruv Tarsadiya; Gregory Yauney; Swabha Swayamdipta
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: We apply our approach to the MATH and AIME reasoning tasks and find animprovement over self-verification and majority vote baselines by up to 6points of accuracy.

172, TITLE: Open Agent Specification (Agent Spec) Technical Report
AUTHORS: Yassine Benajiba; Cesare Bernardis; Vladislav Blinov; Paul Cayet; Hassan Chafi; Abderrahim Fathan; Louis Faucon; Damien Hilloulin; Sungpack Hong; Ingo Kossyk; Rhicheek Patra; Sujith Ravi; Jonas Schweizer; Jyotika Singh; Shailender Singh; Xuelin Situ; Weiyi Sun; Jerry Xu; Ying Xu
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: Agent Spec aims to resolve the challenges of fragmented agent development byproviding a common unified specification that allows AI agents to be designedonce and deployed across various frameworks, improving interoperability andreusability, and reducing redundant development efforts.

173, TITLE: Certifiable Safe RLHF: Fixed-Penalty Constraint Optimization for Safer Language Models
AUTHORS: Kartik Pandit; Sourav Ganguly; Arnesh Banerjee; Shaahin Angizi; Arnob Ghosh
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: First, their reliance on reward and cost functions rendersperformance highly sensitive to the underlying scoring mechanism, which mustcapture semantic meaning rather than being triggered by superficial keywords.Second, CMDP-based training entails tuning dual-variable, a process that isboth computationally expensive and does not provide any provable safetyguarantee for a fixed dual variable that can be exploitable through adversarialjailbreaks. To overcome these limitations, we introduce Certifiable Safe-RLHF(CS-RLHF) that introduces a cost model trained on a large-scale corpus toassign semantically grounded safety scores.

174, TITLE: LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future Directions
AUTHORS: Mizanur Rahman; Amran Bhuiyan; Mohammed Saidul Islam; Md Tahmid Rahman Laskar; Ridwan Mahbub; Ahmed Masry; Shafiq Joty; Enamul Hoque
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: We concludeby outlining open challenges in alignment stability, explainability,governance, and robust evaluation frameworks, and propose future researchdirections to guide the development of robust, trustworthy, low-latency,transparent, and broadly accessible data science agents.

175, TITLE: Thinking on The Fly: Test-Time Reasoning Enhancement Via Latent Thought Policy Optimization
AUTHORS: Wengao Ye; Yan Liang; Lianlei Shan
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: Recent advancements in Large Language Models (LLMs) have shifted fromexplicit Chain-of-Thought (CoT) reasoning to more efficient latent reasoning,where intermediate thoughts are represented as vectors rather than text.However, latent reasoning can be brittle on challenging, out-of-distributiontasks where robust reasoning is most critical. To overcome these limitations,we introduce Latent Thought Policy Optimization (LTPO), a parameter-freeframework that enhances LLM reasoning entirely at test time, without requiringmodel parameter updates.

176, TITLE: Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training
AUTHORS: Wei Xiong; Chenlu Ye; Baohao Liao; Hanze Dong; Xinxing Xu; Christof Monz; Jiang Bian; Nan Jiang; Tong Zhang
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: Prior work such as GVM-RAFTaddresses this by dynamically allocating inference budget per prompt tominimize stochastic gradient variance under a budget constraint. Inspired bythis insight, we propose Reinforce-Ada, an adaptive sampling framework foronline RL post-training of LLMs that continuously reallocates sampling effortto the prompts with the greatest uncertainty or learning potential.

177, TITLE: Human Behavior Atlas: Benchmarking Unified Psychological and Social Behavior Understanding
AUTHORS: Keane Ong; Wei Dai; Carol Li; Dewei Feng; Hengzhi Li; Jingyao Wu; Jiaee Cheong; Rui Mao; Gianmarco Mengaldo; Erik Cambria; Paul Pu Liang
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: On Human BehaviorAtlas, we train three models: OmniSapiens-7B SFT, OmniSapiens-7B BAM, andOmniSapiens-7B RL.

178, TITLE: Partial Information Decomposition Via Normalizing Flows in Latent Gaussian Distributions
AUTHORS: Wenyuan Zhao; Adithya Balachandran; Chao Tian; Paul Pu Liang
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: We propose a new gradient-based algorithm thatsubstantially improves the computational efficiency of GPID based on analternative formulation of the underlying optimization problem.

179, TITLE: DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation Via Noise Injection and Auxiliary Networks
AUTHORS: Nghiem T. Diep; Hien Dang; Tuan Truong; Tan Dinh; Huy Nguyen; Nhat Ho
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: Among these techniques,Weight-Decomposed Low-Rank Adaptation (DoRA) has been shown to improve both thelearning capacity and training stability of the vanilla Low-Rank Adaptation(LoRA) method by explicitly decomposing pre-trained weights into magnitude anddirectional components. In this work, we propose DoRAN, a new variant of DoRAdesigned to further stabilize training and boost the sample efficiency of DoRA.Our approach includes two key stages: (i) injecting noise into the denominatorof DoRA's weight decomposition, which serves as an adaptive regularizer tomitigate instabilities; and (ii) replacing static low-rank matrices withauxiliary networks that generate them dynamically, enabling parameter couplingacross layers and yielding better sample efficiency in both theory andpractice.

180, TITLE: Efficient Quantum Hermite Transform
AUTHORS: Siddhartha Jain; Vishnu Iyer; Rolando D. Somma; Ning Bao; Stephen P. Jordan
CATEGORY: arxiv-quant-ph [ARXIV-QUANT-PH]
HIGHLIGHT: We present a new primitive for quantum algorithms that implements a discreteHermite transform efficiently, in time that depends logarithmically in both thedimension and the inverse of the allowable error.

181, TITLE: Deep Learning-Based Multi-Factor Authentication: A Survey of Biometric and Smart Card Integration Approaches
AUTHORS: Abdelilah Ganmati; Karim Afdel; Lahcen Koutti
CATEGORY: arxiv-cs.CR [CR]
HIGHLIGHT: This survey presents acomprehensive synthesis of recent work (2019-2025) at the intersection of deeplearning, biometrics, and smart card technologies for MFA.

182, TITLE: Model-Guided Microstimulation Steers Primate Visual Behavior
AUTHORS: Johannes Mehrer; Ben Lonnqvist; Anna Mitola; Abdulkadir Gokce; Paolo Papale; Martin Schrimpf
CATEGORY: arxiv-q-bio.NC [NC]
HIGHLIGHT: We here introduce a computational framework to causallymodel and guide stimulation of high-level cortex, comprising three keycomponents: (1) a perturbation module that translates microstimulationparameters into spatial changes to neural activity, (2) topographic models thatcapture the spatial organization of cortical neurons and thus enableprototyping of stimulation experiments, and (3) a mapping procedure that linksmodel-optimized stimulation sites back to primate cortex.

183, TITLE: When Do Credal Sets Stabilize? Fixed-Point Theorems for Credal Set Updates
AUTHORS: Michele Caprio; Siu Lun Chau; Krikamol Muandet
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: This naturally raises the question of whether thisiterative process converges to stable fixed points -- or, more generally, underwhat conditions on the updating mechanism such fixed points exist, and whetherthey can be attained. We provide the first analysis of this problem andillustrate our findings using Credal Bayesian Deep Learning as a concreteexample.

184, TITLE: C3Editor: Achieving Controllable Consistency in 2D Model for 3D Editing
AUTHORS: Zeng Tao; Zheng Ding; Zeyuan Chen; Xiang Zhang; Leizhi Li; Zhuowen Tu
CATEGORY: arxiv-cs.GR [GR]
HIGHLIGHT: Existing 2D-lifting-based 3D editing methods often encounter challengesrelated to inconsistency, stemming from the lack of view-consistent 2D editingmodels and the difficulty of ensuring consistent editing across multiple views.To address these issues, we propose C3Editor, a controllable and consistent2D-lifting-based 3D editing framework.

185, TITLE: Bond-Centered Molecular Fingerprint Derivatives: A BBBP Dataset Study
AUTHORS: Guillaume Godin
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: We introduce a staticBCFP that mirrors the bond-convolution used by directed message-passing GNNslike ChemProp, and evaluate it with a fast rapid Random Forest model onBrain-Blood Barrier Penetration (BBBP) classification task.

186, TITLE: MonitorVLM:A Vision Language Framework for Safety Violation Detection in Mining Operations
AUTHORS: Jiang Wu; Sichao Wu; Yinsong Ma; Guangyuan Yu; Haoyuan Xu; Lifang Zheng; Jingliang Duan
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: In this paper, we presentMonitorVLM, a novel vision--language framework designed to detect safetyviolations directly from surveillance video streams.

187, TITLE: Video Game Level Design As A Multi-Agent Reinforcement Learning Problem
AUTHORS: Sam Earle; Zehua Jiang; Eugene Vinitsky; Julian Togelius
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: Abstract: Procedural Content Generation via Reinforcement Learning (PCGRL) offers amethod for training controllable level designer agents without the need forhuman datasets, using metrics ...

188, TITLE: On The Hardness of Learning Regular Expressions
AUTHORS: Idan Attias; Lev Reyzin; Nathan Srebro; Gal Vardi
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: We study the computational hardness of improperly learning regularexpressions in the PAC model and with membership queries.

189, TITLE: Closing The Loop: Coordinating Inventory and Recommendation Via Deep Reinforcement Learning on Multiple Timescales
AUTHORS: Jinyang Jiang; Jinhui Han; Yijie Peng; Ying Zhang
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: This paper proposes a unified multi-agent RL frameworktailored for joint optimization across distinct functional modules, exemplifiedvia coordinating inventory replenishment and personalized productrecommendation.

190, TITLE: The Bayesian Origin of The Probability Weighting Function in Human Representation of Probabilities
AUTHORS: Xin Tong; Thi Thu Uyen Hoang; Xue-Xin Wei; Michael Hahn
CATEGORY: arxiv-q-bio.NC [NC]
HIGHLIGHT: Here, we present an account of the ProbabilityWeighting Function grounded in rational inference over optimal decoding fromnoisy neural encoding of quantities.

191, TITLE: ConceptSplit: Decoupled Multi-Concept Personalization of Diffusion Models Via Token-wise Adaptation and Attention Disentanglement
AUTHORS: Habin Lim; Yeongseob Won; Juwon Seo; Gyeong-Moon Park
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: The main challenge of this task is "concept mixing", where multiplelearned concepts interfere or blend undesirably in the output image. To addressthis issue, in this paper, we present ConceptSplit, a novel framework to splitthe individual concepts through training and inference.

192, TITLE: Real-time Prediction of Urban Sound Propagation with Conditioned Normalizing Flows
AUTHORS: Achim Eckerle; Martin Spitznagel; Janis Keuper
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: The model reproduces diffraction andinterference patterns and supports instant recomputation under source orgeometry changes, making it a practical engine for urban planning, compliancemapping, and operations (e.g., temporary road closures, night-work varianceassessments).

193, TITLE: Diverse Text-to-Image Generation Via Contrastive Noise Optimization
AUTHORS: Byungjun Kim; Soobin Um; Jong Chul Ye
CATEGORY: arxiv-cs.GR [GR]
HIGHLIGHT: In this work, we introduceContrastive Noise Optimization, a simple yet effective method that addressesthe diversity issue from a distinct perspective.

194, TITLE: The 1st Solution for CARE Liver Task Challenge 2025: Contrast-Aware Semi-Supervised Segmentation with Domain Generalization and Test-Time Adaptation
AUTHORS: Jincan Lou; Jingkun Chen; Haoquan Li; Hang Li; Wenjian Huang; Weihua Chen; Fan Wang; Jianguo Zhang
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: Meanwhile, these methods are originallyused to deal with cross-modality scenarios, and often introduce structuraldistortions and suffer from unstable training, which may pose drawbacks in oursingle-modality scenario. To address these challenges, we propose CoSSeg-TTA, acompact segmentation framework for the GED4 (Gd-EOB-DTPA enhanced hepatobiliaryphase MRI) modality built upon nnU-Netv2 and enhanced with a semi-supervisedmean teacher scheme to exploit large amounts of unlabeled volumes.

195, TITLE: Trade in Minutes! Rationality-Driven Agentic System for Quantitative Financial Trading
AUTHORS: Zifan Song; Kaitao Song; Guosheng Hu; Ding Qi; Junyao Gao; Xiaohua Wang; Dongsheng Li; Cairong Zhao
CATEGORY: arxiv-cs.MA [MA]
HIGHLIGHT: In this paper, we pioneer the harmonization ofstrategic depth in agents with the mechanical rationality essential forquantitative trading.

196, TITLE: SONA: Learning Conditional, Unconditional, and Mismatching-Aware Discriminator
AUTHORS: Yuhta Takida; Satoshi Hayakawa; Takashi Shibuya; Masaaki Imaizumi; Naoki Murata; Bac Nguyen; Toshimitsu Uesaka; Chieh-Hsin Lai; Yuki Mitsufuji
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: Existingconditional generative adversarial networks often struggle to balance the dualobjectives of assessing authenticity and conditional alignment of input sampleswithin their conditional discriminators. To address this, we propose a noveldiscriminator design that integrates three key capabilities: unconditionaldiscrimination, matching-aware supervision to enhance alignment sensitivity,and adaptive weighting to dynamically balance all objectives.

197, TITLE: REG: A Regularization Optimizer for Robust Training Dynamics
AUTHORS: Zehua Liu; Han Wu; Xiaojin Fu; Shuqi Liu; Xiongwei Han; Tao Zhong; Mingxuan Yuan
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: However, Muon's reliance on the matrix sign function canlead to training instability, exhibits incompatibility when fine-tuning modelspre-trained with AdamW. To address these limitations, we propose \textbf{REG},a novel optimizer that replaces Muon's aggressive matrix sign operator with theRow-and-Column-Scaling (RACS) operator.

198, TITLE: BenthiCat: An Opti-acoustic Dataset for Advancing Benthic Classification and Habitat Mapping
AUTHORS: Hayat Rajani; Valerio Franchi; Borja Martinez-Clavel Valles; Raimon Ramos; Rafael Garcia; Nuno Gracias
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: This paper introduces athorough multi-modal dataset, comprising about a million side-scan sonar (SSS)tiles collected along the coast of Catalonia (Spain), complemented bybathymetric maps and a set of co-registered optical images from targetedsurveys using an autonomous underwater vehicle (AUV).

199, TITLE: \textsc{GUI-Spotlight}: Adaptive Iterative Focus Refinement for Enhanced GUI Visual Grounding
AUTHORS: Bin Lei; Nuo Xu; Ali Payani; Mingyi Hong; Chunhua Liao; Yu Cao; Caiwen Ding
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: However, practical usefulness is still bounded by the reliability ofvisual grounding, i.e., mapping textual references to exact on-screen elements.This limitation prevents the system from accurately performing pointer-levelactions such as clicking or dragging. To address it, we introduce GUI-Spotlight-- a model trained for image-grounded reasoning that dynamically invokesmultiple specialized tools to iteratively narrow its focus to the relevantregion of the screen, thereby substantially improving visual groundingaccuracy.

200, TITLE: AI Adoption Across Mission-Driven Organizations
AUTHORS: Dalia Ali; Muneeb Ahmed; Hailan Wang; Arfa Khan; Naira Paola Arnez Jordan; Sunnie S. Y. Kim; Meet Dilip Muchhala; Anne Kathrin Merkle; Orestis Papakyriakopoulos
CATEGORY: arxiv-cs.CY [CY]
HIGHLIGHT: We conducted thematicanalysis of semi-structured interviews with 15 practitioners fromenvironmental, humanitarian, and development organizations across the GlobalNorth and South contexts.

201, TITLE: GA4GC: Greener Agent for Greener Code Via Multi-Objective Configuration Optimization
AUTHORS: Jingzhi Gong; Yixin Bian; Luis de la Cal; Giovanni Pinna; Anisha Uteem; David Williams; Mar Zamorano; Karine Even-Mendoza; W. B. Langdon; Hector Menendez; Federica Sarro
CATEGORY: arxiv-cs.SE [SE]
HIGHLIGHT: Abstract: Coding agents powered by LLMs face critical sustainability and scalabilitychallenges in industrial deployment, with single runs consuming over 100ktokens and incurring ...

202, TITLE: Does Higher Interpretability Imply Better Utility? A Pairwise Analysis on Sparse Autoencoders
AUTHORS: Xu Wang; Yan Hu; Benyou Wang; Difan Zou
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: Sparse Autoencoders (SAEs) are widely used to steer large language models(LLMs), based on the assumption that their interpretable features naturallyenable effective model behavior steering. Yet, a fundamental question remainsunanswered: does higher interpretability indeed imply better steering utility?To answer this question, we train 90 SAEs across three LLMs (Gemma-2-2B,Qwen-2.5-3B, Gemma-2-9B), spanning five architectures and six sparsity levels,and evaluate their interpretability and steering utility based on SAEBench(arXiv:2501.12345) and AxBench (arXiv:2502.23456) respectively, and perform arank-agreement analysis via Kendall's rank coefficients (tau b).

203, TITLE: Deep Learning The Sources of MJO Predictability: A Spectral View of Learned Features
AUTHORS: Lin Yao; Da Yang; James P. C. Duncan; Ashesh Chattopadhyay; Pedram Hassanzadeh; Wahid Bhimji; Bin Yu
CATEGORY: arxiv-physics.ao-ph [AO-PH]
HIGHLIGHT: To identify the spatial scalesmost relevant for MJO forecasting, we conduct spectral analysis of the latentfeature space and find that large-scale patterns dominate the learned signals.Additional experiments show that models using only large-scale signals as theinput have the same skills as those using all the scales, supporting thelarge-scale view of the MJO.

204, TITLE: Focused Skill Discovery: Learning to Control Specific State Variables While Minimizing Side Effects
AUTHORS: Jonathan Colaço Carr; Qinyi Sun; Cameron Allen
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: This cansignificantly hamper exploration efficiency, make skills more challenging tolearn with, and lead to negative side effects in downstream tasks when the goalis under-specified. We introduce a general method that enables these skilldiscovery algorithms to learn focused skills -- skills that target and controlspecific state variables.

205, TITLE: AgentHub: A Research Agenda for Agent Sharing Infrastructure
AUTHORS: Erik Pautsch; Tanmay Singla; Wenxin Jiang; Huiyun Peng; Behnaz Hassanshahi; Konstantin Läufer; George K. Thiruvathukal; James C. Davis
CATEGORY: arxiv-cs.SE [SE]
HIGHLIGHT: However, considering broadersoftware engineering requirements would improve open-source distribution andease reuse. We therefore propose AgentHub, a research agenda for agent sharing.By framing the key challenges of capability clarity, lifecycle transparency,interoperability, governance, security, and workflow integration, AgentHubcharts a community-wide agenda for building reliable and scalable agentecosystems.

206, TITLE: Fine-Tuning on Noisy Instructions: Effects on Generalization and Performance
AUTHORS: Ahmed Alajrami; Xingwei Tan; Nikolaos Aletras
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: Instruction-tuning plays a vital role in enhancing the task-solving abilitiesof large language models (LLMs), improving their usability in generatinghelpful responses on various tasks.

207, TITLE: Diffusion^2: Dual Diffusion Model with Uncertainty-Aware Adaptive Noise for Momentary Trajectory Prediction
AUTHORS: Yuhao Luo; Yuang Zhang; Kehua Chen; Xinyu Zheng; Shucheng Zhang; Sikai Chen; Yinhai Wang
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: In this work, we propose a novel framework termedDiffusion^2, tailored for momentary trajectory prediction.

208, TITLE: REAR: Rethinking Visual Autoregressive Models Via Generator-Tokenizer Consistency Regularization
AUTHORS: Qiyuan He; Yicong Li; Haotian Ye; Jinghao Wang; Xinyao Liao; Pheng-Ann Heng; Stefano Ermon; James Zou; Angela Yao
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: In this work, we identify a core bottleneck fromthe perspective of generator-tokenizer inconsistency, i.e., the AR-generatedtokens may not be well-decoded by the tokenizer.

209, TITLE: Mitigating Forgetting Between Supervised and Reinforcement Learning Yields Stronger Reasoners
AUTHORS: Xiangchi Yuan; Xiang Chen; Tong Yu; Dachuan Shi; Can Jin; Wenke Lee; Saayan Mitra
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: Large Language Models (LLMs) show strong reasoning abilities, often amplifiedby Chain-of-Thought (CoT) prompting and reinforcement learning (RL).

210, TITLE: Integrating Ontology and Computer Vision for Intelligent Monitoring of Unsafe Conditions in Hot Work
AUTHORS: Zhengwen Zhou; Shan Chen; Junhui Kou; Siqi Chen; Jiaxin Liu; Liangjie Guo
CATEGORY: Automation in Construction [AUTOMATION IN CONSTRUCTION]
HIGHLIGHT: 

211, TITLE: SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful Requests
AUTHORS: Punya Syon Pandey; Hai Son Le; Devansh Bhardwaj; Rada Mihalcea; Zhijing Jin
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: We introduce SocialHarmBench, a dataset of 585 promptsspanning 7 sociopolitical categories and 34 countries, designed to surfacewhere LLMs most acutely fail in politically charged contexts.

212, TITLE: Agile Tradespace Exploration for Space Rendezvous Mission Design Via Transformers
AUTHORS: Yuji Takubo; Daniele Gammelli; Marco Pavone; Simone D'Amico
CATEGORY: arxiv-math.OC [OC]
HIGHLIGHT: Given the orbital information of thetarget spacecraft, boundary conditions, and a range of flight times, this workproposes a Transformer-based architecture that generates, in a singleparallelized inference step, a set of near-Pareto optimal trajectories acrossvarying flight times, thereby enabling rapid mission trade studies.

213, TITLE: Did You Just See That? Arbitrary View Synthesis for Egocentric Replay of Operating Room Workflows from Ambient Sensors
AUTHORS: Han Zhang; Lalithkumar Seenivasan; Jose L. Porras; Roger D. Soberanis-Mukul; Hao Ding; Hongchao Shu; Benjamin D. Killeen; Ankita Ghosh; Lonny Yarmus; Masaru Ishii; Angela Christine Argento; Mathias Unberath
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: Here we introduce EgoSurg, the first framework to reconstruct thedynamic, egocentric replays for any operating room (OR) staff directly fromwall-mounted fixed-camera video, and thus, without intervention to clinicalworkflow.

214, TITLE: Identifying Financial Risk Information Using RAG with A Contrastive Insight
AUTHORS: Ali Elahi
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: When applying reasoning in specialized contexts withLLMs on top of a RAG, the pipeline can capture contextually relevantinformation, but it is not designed to retrieve comparable cases or relatedproblems.

215, TITLE: PolyKAN: A Polyhedral Analysis Framework for Provable and Approximately Optimal KAN Compression
AUTHORS: Di Zhang
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: By leveragingthe inherent piecewise polynomial structure of KANs, we formulate thecompression problem as a polyhedral region merging task.

216, TITLE: Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents
AUTHORS: Muyu He; Anand Kumar; Tsach Mackey; Meghana Rajeev; James Zou; Nazneen Rajani
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: UsingTraitBasis, we extend $\tau$-Bench to $\tau$-Trait, where user behaviors arealtered via controlled trait vectors.

217, TITLE: Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole Slide Image Diagnosis Behavior
AUTHORS: Sheng Wang; Ruiming Wu; Charles Herndon; Yihang Liu; Shunsuke Koga; Jeanne Shen; Zhi Huang
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: Turning everyday viewer logs into scalable, expert-validatedsupervision, our framework makes agentic pathology practical and establishes apath to human-aligned, upgradeable clinical AI.

218, TITLE: Asynchronous Denoising Diffusion Models for Aligning Text-to-Image Generation
AUTHORS: Zijing Hu; Yunze Tong; Fengda Zhang; Junkun Yuan; Jun Xiao; Kun Kuang
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: As a result,during generation, the prompt-related regions can only reference the unrelatedregions at the same noise level, failing to obtain clear context and ultimatelyimpairing text-to-image alignment. To address this issue, we proposeasynchronous diffusion models -- a novel framework that allocates distincttimesteps to different pixels and reformulates the pixel-wise denoisingprocess.

219, TITLE: Cross-Modal Content Optimization for Steering Web Agent Preferences
AUTHORS: Tanqiu Jiang; Min Bai; Nikolaos Pappas; Yanjun Qi; Sandesh Swamy
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: In this paper, we demonstrate, for the first time, that jointexploitation of visual and textual channels yields significantly more powerfulpreference manipulations under realistic attacker capabilities.

220, TITLE: Fine-Tuning Large Language Models with QLoRA for Offensive Language Detection in Roman Urdu-English Code-Mixed Text
AUTHORS: Nisar Hussain; Amna Qasim; Gull Mehak; Muhammad Zain; Momina Hafeez; Grigori Sidorov
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: Inthis work, we propose a QLoRA based fine tuning framework to improve offensivelanguage detection in Roman Urdu-English text.

221, TITLE: Spatial-ViLT: Enhancing Visual Spatial Reasoning Through Multi-Task Learning
AUTHORS: Chashi Mahiul Islam; Oteo Mamo; Samuel Jacob Chacko; Xiuwen Liu; Weikuan Yu
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: Vision-language models (VLMs) have advanced multimodal reasoning but stillface challenges in spatial reasoning for 3D scenes and complex objectconfigurations. To address this, we introduce SpatialViLT, an enhanced VLM thatintegrates spatial features like depth maps, 3D coordinates, and edge mapsthrough a multi-task learning framework.

222, TITLE: Optimal Scaling Needs Optimal Norm
AUTHORS: Oleg Filatov; Jiangtao Wang; Jan Ebert; Stefan Kesselheim
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: Usingthe Scion optimizer, we discover that joint optimal scaling across model anddataset sizes is governed by a single invariant: the operator norm of theoutput layer.

223, TITLE: Adaptive Double-phase Rudin--Osher--Fatemi Denoising Model
AUTHORS: Wojciech Górny; Michał Łasica; Alexandros Matsoukas
CATEGORY: arxiv-eess.IV [IV]
HIGHLIGHT: We propose a new image denoising model based on a variable-growth totalvariation regularization of double-phase type with adaptive weight.

224, TITLE: GenAR: Next-Scale Autoregressive Generation for Spatial Gene Expression Prediction
AUTHORS: Jiarui Ouyang; Yihui Wang; Yihang Gao; Yingxue Xu; Shu Yang; Hao Chen
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: Abstract: Spatial Transcriptomics (ST) offers spatially resolved gene expression butremains costly. Predicting expression directly from widely availableHematoxylin and Eosin (H&E) stained ...

225, TITLE: 6G- Enabled Digital Twin Framework for Real-Time Cyber-Physical Systems: An Experimental Validation with Industrial Bearing Fault Detection
AUTHORS: Vaskar Chakma; Wooyeol Choi
CATEGORY: arxiv-cs.NI [NI]
HIGHLIGHT: This research aims to develop and validate a 6G-enabledDigital Twin framework that achieves ultra-low latency communication andreal-time synchronization between physical industrial assets and their digitalcounterparts, specifically targeting bearing fault detection as a criticalindustrial use case.

226, TITLE: Read The Scene, Not The Script: Outcome-Aware Safety for LLMs
AUTHORS: Rui Wu; Yihao Quan; Zeru Shi; Zhenting Wang; Yanshu Li; Ruixiang Tang
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: To mitigateconsequence-blindness, we introduce CS-Chain-4k, a consequence-reasoningdataset for safety alignment.

227, TITLE: What Shapes A Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models
AUTHORS: Zicong He; Boxuan Zhang; Weihao Liu; Ruixiang Tang; Lu Cheng
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: Through extensive experiments on leading proprietary and open-sourcemodels, we analyze trade-offs in their creative capabilities.

228, TITLE: Bidirectional Mammogram View Translation with Column-Aware and Implicit 3D Conditional Diffusion
AUTHORS: Xin Li; Kaixiang Yang; Qiang Li; Zhiwei Wang
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: In this paper, we propose Column-Aware andImplicit 3D Diffusion (CA3D-Diff), a novel bidirectional mammogram viewtranslation framework based on conditional diffusion model.

229, TITLE: Conditional Representation Learning for Customized Tasks
AUTHORS: Honglin Liu; Chao Sun; Peng Hu; Yunfan Li; Xi Peng
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: In this paper, we propose ConditionalRepresentation Learning (CRL), aiming to extract representations tailored toarbitrary user-specified criteria.

230, TITLE: Searching for The Most Human-like Emergent Language
AUTHORS: Brendon Boldt; David Mortensen
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: In this paper, we design a signalling game-based emergent communicationenvironment to generate state-of-the-art emergent languages in terms ofsimilarity to human language.

231, TITLE: Morpheme Induction for Emergent Language
AUTHORS: Brendon Boldt; David Mortensen
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: We introduce CSAR, an algorithm for inducing morphemes from emergent languagecorpora of parallel utterances and meanings.

232, TITLE: Retrieval-Augmented Code Generation: A Survey with Focus on Repository-Level Approaches
AUTHORS: Yicheng Tao; Yao Qin; Yepang Liu
CATEGORY: arxiv-cs.SE [SE]
HIGHLIGHT: In this survey, we provide a comprehensivereview of research on Retrieval-Augmented Code Generation (RACG), with anemphasis on repository-level approaches.

233, TITLE: Inoculation Prompting: Eliciting Traits from LLMs During Training Can Suppress Them at Test-time
AUTHORS: Daniel Tan; Anders Woodruff; Niels Warncke; Arun Jose; Maxime Riché; David Demitri Africa; Mia Taylor
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: Language model finetuning often results in learning undesirable traits incombination with desired ones. To address this, we propose inoculationprompting: modifying finetuning data by prepending a short system-promptinstruction that deliberately elicits the undesirable trait.

234, TITLE: Keep It on A Leash: Controllable Pseudo-label Generation Towards Realistic Long-Tailed Semi-Supervised Learning
AUTHORS: Yaxin Hou; Bo Han; Yuheng Jia; Hui Liu; Junhui Hou
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: Abstract: Current long-tailed semi-supervised learning methods assume that labeled dataexhibit a long-tailed distribution, and unlabeled data adhere to a typicalpredefined distribution ...

235, TITLE: Doctor-R1: Mastering Clinical Inquiry with Experiential Agentic Reinforcement Learning
AUTHORS: Yunghwei Lai; Kaiming Liu; Ziyue Wang; Weizhi Ma; Yang Liu
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: However, they often lack the ability to conduct thestrategic and empathetic consultation, which is essential for real-worldclinical scenarios. To address this gap, we propose Doctor-R1, an AI doctoragent trained to master both of the capabilities by ask high-yield questionsand conduct strategic multi-turn inquiry to guide decision-making.

236, TITLE: Safe and Compliant Cross-Market Trade Execution Via Constrained RL and Zero-Knowledge Audits
AUTHORS: Ailiya Borjigin; Cong He
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: We present a cross-market algorithmic trading system that balances executionquality with rigorous compliance enforcement.

237, TITLE: Can An LLM Induce A Graph? Investigating Memory Drift and Context Length
AUTHORS: Raquib Bin Yousuf; Aadyant Khatri; Shengzhe Xu; Mandar Sharma; Naren Ramakrishnan
CATEGORY: arxiv-cs.CL [CL]
HIGHLIGHT: However, these benchmarks often rely on simplistic 'needle in a haystack'retrieval or continuation tasks that may not accurately reflect the performanceof these models in information-dense scenarios. Thus, rather than simple nexttoken prediction, we argue for evaluating these models on more complexreasoning tasks that requires them to induce structured relational knowledgefrom the text - such as graphs from potentially noisy natural language content.While the input text can be viewed as generated in terms of a graph, itsstructure is not made explicit and connections must be induced from distributedtextual cues, separated by long contexts and interspersed with irrelevantinformation.

238, TITLE: Your Vision-Language Model Can't Even Count to 20: Exposing The Failures of VLMs in Compositional Counting
AUTHORS: Xuyang Guo; Zekai Huang; Zhenmei Shi; Zhao Song; Jiahao Zhang
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: In this paper, we introduce a simple yet effective benchmark,VLMCountBench, designed under a minimalist setting with only basic geometricshapes (e.g., triangles, circles) and their compositions, focusing exclusivelyon counting tasks without interference from other factors.

239, TITLE: Joint Learning of Pose Regression and Denoising Diffusion with Score Scaling Sampling for Category-level 6D Pose Estimation
AUTHORS: Seunghyun Lee; Tae-Kyun Kim
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: The existing methods, however, suffer from slow convergence duringtraining, learning its encoder with the diffusion denoising network inend-to-end fashion, and require an additional network that evaluates sampledpose hypotheses to filter out low-quality pose candidates. In this paper, wepropose a novel pipeline that tackles these limitations by two key components.First, the proposed method pretrains the encoder with the direct poseregression head, and jointly learns the networks via the regression head andthe denoising diffusion head, significantly accelerating training convergencewhile achieving higher accuracy.

240, TITLE: Proactive Defense Against LLM Jailbreak
AUTHORS: Weiliang Zhao; Jinjun Peng; Daniel Ben-Levi; Zhou Yu; Junfeng Yang
CATEGORY: arxiv-cs.CR [CR]
HIGHLIGHT: In this paper, we introduce ProAct,a novel proactive defense framework designed to disrupt and mislead autonomousjailbreaking processes.

241, TITLE: ESE-UNet: An Extremely Simple Yet Efficient UNet for Medical Image Segmentation
AUTHORS: Xiang Yu; Meiling Liang; Qing Zeng; Zeyu Ren; Cheng Kang; Cheng Jiang; Guanyuan Chen; Bocheng Liang; Bin Pu; Ningbo Zhu; Guanghua Tan; Feng Liu; Yuanshen Zhao; Shuyuan Ouyang; Shengli Li; Ying Yuan
CATEGORY: Biomedical Signal Processing and Control [BIOMEDICAL SIGNAL PROCESSING AND CONTROL]
HIGHLIGHT: 

242, TITLE: Perfect AI Mimicry and The Epistemology of Consciousness: A Solipsistic Dilemma
AUTHORS: Shurui Li
CATEGORY: arxiv-cs.AI [AI]
HIGHLIGHT: As AIsystems increasingly mimic human behavior and interaction with high fidelity,the concept of a "perfect mimic"-an entity empirically indistinguishable from ahuman through observation and interaction-shifts from hypothetical totechnologically plausible. This paper argues that such developments pose afundamental challenge to the consistency of our mind-recognition practices.Consciousness attributions rely heavily, if not exclusively, on empiricalevidence derived from behavior and interaction.

243, TITLE: Diffusion-Classifier Synergy: Reward-Aligned Learning Via Mutual Boosting Loop for FSCIL
AUTHORS: Ruitao Wu; Yifan Zhao; Guangyao Chen; Jia Li
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: This paper introduces Diffusion-Classifier Synergy (DCS), a novelframework that establishes a mutual boosting loop between diffusion model andFSCIL classifier.

244, TITLE: The NPA Hierarchy Does Not Always Attain The Commuting Operator Value
AUTHORS: Marco Fanizza; Larissa Kroell; Arthur Mehta; Connor Paddock; Denis Rochette; William Slofstra; Yuming Zhao
CATEGORY: arxiv-quant-ph [ARXIV-QUANT-PH]
HIGHLIGHT: Our contribution involves establishing acomputable mapping from Turing machines to BCS nonlocal games in which thehalting property of the machine is encoded as a decision problem for thecommuting operator value of the game.

245, TITLE: PatternKV: Flattening KV Representation Expands Quantization Headroom
AUTHORS: Ji Zhang; Yiwei Li; Shaoxiong Feng; Peiwen Yuan; Xinglin Wang; Jiayi Shi; Yueqi Zhang; Chuyi Tan; Boyuan Pan; Yao Hu; Kan Li
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: Priorwork focuses on isolating outliers, which caps their error but fails to flattenthe overall distribution, leaving performance fragile under low-bit settings.In this work, we show that the K cache maintains a stable structure thatevolves gradually with context, while the V cache carries latent semanticregularities. Building on these insights, we propose PatternKV, apattern-aligned residual quantization scheme.

246, TITLE: Slow-Fast Policy Optimization: Reposition-Before-Update for LLM Reasoning
AUTHORS: Ziyan Wang; Zheng Wang; Jie Fu; Xingwei Qu; Qi Cheng; Shengpu Tang; Minjia Zhang; Xiaoming Huo
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: Reinforcement learning (RL) has become central to enhancing reasoning inlarge language models (LLMs).

247, TITLE: Super-resolution Image Projection Over An Extended Depth of Field Using A Diffractive Decoder
AUTHORS: Hanlong Chen; Cagatay Isil; Tianyi Gan; Mona Jarrahi; Aydogan Ozcan
CATEGORY: arxiv-physics.optics [OPTICS]
HIGHLIGHT: Here, we introduce a hybrid image projection system that achievesextended depth-of-field (DOF) with improved resolution, combining aconvolutional neural network (CNN)-based digital encoder with an all-opticaldiffractive decoder.

248, TITLE: Unlocking Reasoning Capabilities in LLMs Via Reinforcement Learning Exploration
AUTHORS: Wenhao Deng; Long Wei; Chenglei Yu; Tailin Wu
CATEGORY: arxiv-cs.LG [LG]
HIGHLIGHT: We attribute thisphenomenon to the widespread use of the reverse Kullback-Leibler (KL)divergence regularizer, whose mode-seeking behavior keeps the policy trappedinside the base model's support region and hampers wider exploration. Toaddress this issue, we propose RAPO (Rewards-Aware Policy Optimization), analgorithm to promote broader yet focused exploration.

249, TITLE: Fairness in Repeated Matching: A Maximin Perspective
AUTHORS: Eugene Lim; Tzeh Yuan Neoh; Nicholas Teh
CATEGORY: arxiv-cs.GT [GT]
HIGHLIGHT: We study a sequential decision-making model where a set of items isrepeatedly matched to the same set of agents over multiple rounds.

250, TITLE: ExposureEngine: Oriented Logo Detection and Sponsor Visibility Analytics in Sports Broadcasts
AUTHORS: Mehdi Houshmand Sarkhoosh; Frøy Øye; Henrik Nestor Sørlie; Nam Hoang Vu; Dag Johansen; Cise Midoglu; Tomas Kupka; Pål Halvorsen
CATEGORY: arxiv-cs.CV [CV]
HIGHLIGHT: This paper introduces ExposureEngine, an end-to-endsystem designed for accurate, rotation-aware sponsor visibility analytics insports broadcasts, demonstrated in a soccer case study.

